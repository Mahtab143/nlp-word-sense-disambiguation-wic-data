{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
  	"collapsed": false,
  	"pycharm": {
  	 "name": "#%% md\n"
  	}
   },
   "source": [
  	"<a href=\"https://colab.research.google.com/github/andrea-gasparini/nlp-word-sense-disambiguation-wic-data/blob/master/hw3/stud/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Sense Disambiguation (WSD) of Word-in-Context (WiC) data\n",
    "\n",
    "Third homework of the Natural Language Processing course 2021 @ Sapienza University of Rome.\n",
    "\n",
    "Prof. Roberto Navigli\n",
    "\n",
    "MSc in Computer Science\n",
    "\n",
    "**Author**: Andrea Gasparini - 1813486"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stud import constants, utils\n",
    "from stud.data import HParams, Pos, Token\n",
    "from stud.datasets import WSDDataset, GlossBERTDataset\n",
    "from stud.sense_inventories import build_senses_vocab, SenseInventory\n",
    "from stud.data_readers import read_wic_corpus, read_wsd_gold_keys, read_wsd_corpus\n",
    "from stud.pl_data_modules import WSDDataModule, GlossBERTDataModule\n",
    "from stud.transformer_embedder import TransformerEmbedder\n",
    "from stud.pl_modules import WordSenseDisambiguator, GlossBERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WSDisambiguator datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:04<00:00, 24.29it/s]\n",
      "100%|██████████| 1802/1802 [00:19<00:00, 92.70it/s]\n"
     ]
    }
   ],
   "source": [
    "training_corpus = read_wsd_corpus(f\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t\tf\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "semeval2007_corpus = read_wsd_corpus(f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t         \t\t\t f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "evaluation_corpus = read_wsd_corpus(f\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t\tf\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "\n",
    "wic_samples_dev = read_wic_corpus(constants.WIC_TEST_SET_PATH.replace('../..', '..'), constants.WIC_TEST_SET_WSD_KEYS_PATH.replace('../..', '..'))\n",
    "wic_corpus_dev = list()\n",
    "for wic_sample in wic_samples_dev:\n",
    "\twic_corpus_dev.append(wic_sample.sentence1)\n",
    "\twic_corpus_dev.append(wic_sample.sentence2)\n",
    "\n",
    "wic_samples_train = read_wic_corpus(constants.WIC_TRAIN_SET_PATH.replace('../..', '..'))\n",
    "wic_corpus_train = list()\n",
    "for wic_sample in wic_samples_train:\n",
    "\twic_corpus_train.append(wic_sample.sentence1)\n",
    "\twic_corpus_train.append(wic_sample.sentence2)\n",
    "\n",
    "sense_invent = SenseInventory(constants.GLOSSES_PATH.replace('../..', '..'), constants.LEMMA_POS_DICT_PATH.replace('../..', '..'))\n",
    "senses_vocabulary = build_senses_vocab(training_corpus + evaluation_corpus + wic_corpus_dev)\n",
    "\n",
    "embedder_model = utils.get_pretrained_model(constants.TRANSFORMER_EMBEDDER_PATH.replace('../..', '..'))\n",
    "\n",
    "train_set = WSDDataset(training_corpus, embedder_model, sense_invent, senses_vocabulary)\n",
    "torch.save(train_set, constants.PREPROCESSED_TRAIN_PATH.replace('../..', '..'))\n",
    "\n",
    "valid_set = WSDDataset(semeval2007_corpus, embedder_model, sense_invent, senses_vocabulary)\n",
    "torch.save(valid_set, constants.PREPROCESSED_VALID_PATH.replace('../..', '..'))\n",
    "\n",
    "test_set = WSDDataset(wic_corpus_dev, embedder_model, sense_invent, senses_vocabulary)\n",
    "torch.save(test_set, constants.PREPROCESSED_TEST_PATH.replace('../..', '..'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GlossBERT datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18149/18149 [00:39<00:00, 456.75it/s]\n"
     ]
    }
   ],
   "source": [
    "training_corpus = read_wsd_corpus(f\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t  f\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "semeval2007_corpus = read_wsd_corpus(f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t         \t\t\t f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "evaluation_corpus = read_wsd_corpus(f\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t\tf\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "\n",
    "wic_samples_dev = read_wic_corpus(constants.WIC_TEST_SET_PATH.replace('../..', '..'), constants.WIC_TEST_SET_WSD_KEYS_PATH.replace('../..', '..'))\n",
    "wic_corpus_dev = list()\n",
    "for wic_sample in wic_samples_dev:\n",
    "\twic_corpus_dev.append(wic_sample.sentence1)\n",
    "\twic_corpus_dev.append(wic_sample.sentence2)\n",
    "\n",
    "sense_invent = SenseInventory(constants.GLOSSES_PATH.replace('../..', '..'), constants.LEMMA_POS_DICT_PATH.replace('../..', '..'))\n",
    "\n",
    "SemCor_fraction_size = 0.5\n",
    "SemCor_fraction = random.sample(training_corpus, int(len(training_corpus) * SemCor_fraction_size))\n",
    "\n",
    "train_set = GlossBERTDataset.from_tokens(SemCor_fraction, sense_invent)\n",
    "train_set.save_as_json(f\"../data/preprocessed/GlossBERT/SemCor{int(SemCor_fraction_size * 100)}.json\")\n",
    "\n",
    "valid_set = GlossBERTDataset.from_tokens(semeval2007_corpus, sense_invent)\n",
    "valid_set.save_as_json(constants.PREPROCESSED_GLOSSBERT_VALID_PATH.replace('../..', '..'), indent=4)\n",
    "\n",
    "test_set = GlossBERTDataset.from_tokens(wic_corpus_dev, sense_invent)\n",
    "test_set.save_as_json(constants.PREPROCESSED_GLOSSBERT_TEST_PATH.replace('../..', '..'), indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WSDisambiguator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mandreagasparini\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/hw3/wandb/run-20220612_174029-1ubgvpko</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/andreagasparini/nlp_hw3/runs/1ubgvpko\" target=\"_blank\">WSDisambiguator</a></strong> to <a href=\"https://wandb.ai/andreagasparini/nlp_hw3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | senses_vocab  | Vocab            | 0     \n",
      "1 | model         | Sequential       | 15.8 M\n",
      "2 | loss_function | CrossEntropyLoss | 0     \n",
      "3 | train_acc     | Accuracy         | 0     \n",
      "4 | train_f1      | F1Score          | 0     \n",
      "5 | valid_acc     | Accuracy         | 0     \n",
      "6 | valid_f1      | F1Score          | 0     \n",
      "---------------------------------------------------\n",
      "15.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.022    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "143d8db97040498f9db26173e40366dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c050076982240388f71aa81b9f13b66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "utils.nltk_downloads()\n",
    "\n",
    "sense_inventory = SenseInventory(constants.GLOSSES_PATH.replace(\"../..\", \"..\"), constants.LEMMA_POS_DICT_PATH.replace(\"../..\", \"..\"))\n",
    "\n",
    "data_module = WSDDataModule(constants.PREPROCESSED_TRAIN_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\tconstants.PREPROCESSED_VALID_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\tconstants.PREPROCESSED_TEST_PATH.replace(\"../..\", \"..\"),\n",
    "                            utils.get_pretrained_model(constants.TRANSFORMER_EMBEDDER_PATH.replace(\"../..\", \"..\")),\n",
    "\t\t\t\t\t\t\tsense_inventory,\n",
    "\t\t\t\t\t\t\ttorch.load(\"../model/vocabularies/senses_vocabulary_train.pt\"),\n",
    "\t\t\t\t\t\t\tbatch_size=16,\n",
    "\t\t\t\t\t\t\tnum_workers=4,\n",
    "\t\t\t\t\t\t\tpin_memory=True)\n",
    "\n",
    "hparams = HParams(num_classes=len(data_module.senses_vocab),\n",
    "                  input_size=constants.TRANSFORMER_EMBEDDER_DIMENSION,\n",
    "                  batch_size=data_module.batch_size)\n",
    "\n",
    "MODELS_DIR = \"../model/\"\n",
    "MODEL_NAME = \"WSDisambiguator\"\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor=\"valid_wsd_accuracy\",\n",
    "                                            patience=5,\n",
    "                                            verbose=True,\n",
    "                                            mode=\"max\")\n",
    "\n",
    "check_point_callback = pl.callbacks.ModelCheckpoint(monitor=\"valid_wsd_accuracy\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_top_k=2,\n",
    "                                                    save_last=False,\n",
    "                                                    mode=\"max\",\n",
    "                                                    dirpath=MODELS_DIR,\n",
    "                                                    filename=MODEL_NAME + \"-{epoch}-{valid_wsd_loss:.4f}-\"\n",
    "                                                                          \"{valid_wsd_f1:.3f}-{valid_wsd_accuracy:.3f}\")\n",
    "\n",
    "wandb_logger = WandbLogger(offline=False, project=\"nlp_hw3\", name=MODEL_NAME)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    logger=wandb_logger,\n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=100,\n",
    "    callbacks=[early_stopping, check_point_callback],\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "model = WordSenseDisambiguator(hparams.as_dict(), data_module.senses_vocab, data_module.senses_vocab[constants.UNK_TOKEN])\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GlossBERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mandreagasparini\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/hw3/wandb/run-20220615_002822-2oofd11s</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/andreagasparini/nlp_hw3/runs/2oofd11s\" target=\"_blank\">GlossDistilBERT</a></strong> to <a href=\"https://wandb.ai/andreagasparini/nlp_hw3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                | Params\n",
      "------------------------------------------------------------\n",
      "0 | bert                | TransformerEmbedder | 65.2 M\n",
      "1 | classification_head | Linear              | 769   \n",
      "2 | loss_function       | BCELoss             | 0     \n",
      "3 | train_acc           | Accuracy            | 0     \n",
      "4 | train_f1            | F1Score             | 0     \n",
      "5 | valid_acc           | Accuracy            | 0     \n",
      "6 | valid_f1            | F1Score             | 0     \n",
      "------------------------------------------------------------\n",
      "65.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "65.2 M    Total params\n",
      "260.767   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5314ffb7ce584bef96e2f3b1a9714261"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa9730d8a83a420b84103b7e29d2fef0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "utils.nltk_downloads()\n",
    "\n",
    "sense_inventory = SenseInventory(constants.GLOSSES_PATH.replace('../..', '..'), constants.LEMMA_POS_DICT_PATH.replace('../..', '..'))\n",
    "\n",
    "data_module = GlossBERTDataModule(constants.PREPROCESSED_GLOSSBERT_TRAIN_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\t      constants.PREPROCESSED_GLOSSBERT_VALID_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\t      constants.PREPROCESSED_GLOSSBERT_TEST_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\t      batch_size=8,\n",
    "\t\t\t\t\t\t\t      num_workers=4,\n",
    "\t\t\t\t\t\t\t      pin_memory=True)\n",
    "\n",
    "hparams = HParams(num_classes=1, learning_rate=2e-5, batch_size=data_module.batch_size, input_size=None)\n",
    "\n",
    "MODELS_DIR = \"../model/\"\n",
    "MODEL_NAME = \"GlossBERT\"\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor=\"valid_wsd_accuracy\",\n",
    "                                            patience=5,\n",
    "                                            verbose=True,\n",
    "                                            mode=\"max\")\n",
    "\n",
    "check_point_callback = pl.callbacks.ModelCheckpoint(monitor=\"valid_wsd_accuracy\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_top_k=4,\n",
    "                                                    save_last=False,\n",
    "                                                    mode=\"max\",\n",
    "                                                    dirpath=MODELS_DIR,\n",
    "                                                    filename=MODEL_NAME + \"-{epoch}-{valid_wsd_loss:.4f}-\"\n",
    "                                                                          \"{valid_wsd_f1:.3f}-{valid_wsd_accuracy:.3f}\")\n",
    "\n",
    "wandb_logger = WandbLogger(offline=False, project=\"nlp_hw3\", name=MODEL_NAME)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    logger=wandb_logger,\n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=10,\n",
    "    callbacks=[early_stopping, check_point_callback],\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "model = GlossBERT(hparams.as_dict(), constants.TRANSFORMER_EMBEDDER_PATH.replace('../..', '..'))\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mode = \"GlossBERT\" # \"WSDisambiguator\" | \"GlossBERT\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "bert_model_name = utils.get_pretrained_model(constants.TRANSFORMER_EMBEDDER_PATH)\n",
    "\n",
    "if mode == \"WSDisambiguator\":\n",
    "    vocab = torch.load(\"../model/vocabularies/senses_vocabulary_train.pt\")\n",
    "    wic_dataset = WSDDataset.from_preprocessed(constants.PREPROCESSED_TEST_PATH.replace('../..', '..'))\n",
    "    wic_dataset.to_device(model.device)\n",
    "else:\n",
    "    vocab = torch.load(\"../model/vocabularies/senses_vocabulary_full.pt\")\n",
    "    # wic_dataset = GlossBERTDataset.from_json(constants.PREPROCESSED_GLOSSBERT_VALID_PATH.replace('../..', '..'))\n",
    "    wic_dataset = GlossBERTDataset.from_json(constants.PREPROCESSED_GLOSSBERT_TEST_PATH.replace('../..', '..'))\n",
    "    # wic_dataset = GlossBERTDataset.from_json(\"../data/preprocessed/GlossBERT/wic_dev_no_pos.json\")\n",
    "\n",
    "vocab_itos = vocab.get_itos()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "if mode == \"WSDisambiguator\":\n",
    "    MODEL_PATH = \"../model/WSDisambiguator.ckpt\"\n",
    "    model = WordSenseDisambiguator.load_from_checkpoint(MODEL_PATH, senses_vocab=vocab)\n",
    "else:\n",
    "    MODEL_PATH = \"../model/GlossBERT.ckpt\"\n",
    "    model = GlossBERT.load_from_checkpoint(MODEL_PATH, bert_model_name_or_path=\"../model/bert-base-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: 100%|█████████▉| 8617/8622 [15:53<00:01,  4.03it/s]\u001B[A"
     ]
    }
   ],
   "source": [
    "model.freeze()\n",
    "predictions = list()\n",
    "predictions_indices = list()\n",
    "\n",
    "if mode == \"WSDisambiguator\":\n",
    "    progress_bar = tqdm(total=len(wic_dataset), desc='Evaluating')\n",
    "    for batch in DataLoader(wic_dataset, batch_size=8, collate_fn=WSDDataset.collate_fn):\n",
    "        out = model(batch)\n",
    "        predicted_indices, predicted_ids = model.predict(batch, out)\n",
    "        predictions += predicted_ids\n",
    "        predictions_indices += predicted_indices\n",
    "        progress_bar.update(len(batch))\n",
    "else:\n",
    "    tokens = list()\n",
    "    probs = list()\n",
    "    sense_indices = list()\n",
    "\n",
    "    progress_bar = tqdm(total=len(wic_dataset), desc='Evaluating')\n",
    "\n",
    "    dataloader = DataLoader(wic_dataset, batch_size=8, collate_fn=GlossBERTDataset.collate_fn)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        out = model(batch)\n",
    "        for i in range(len(batch[\"tokens\"])):\n",
    "            token = batch[\"tokens\"][i]\n",
    "            prob = out[\"probabilities\"][i]\n",
    "            sense_index = batch[\"sense_indices\"][i]\n",
    "            if len(tokens) > 0 and token == tokens[-1][-1]:\n",
    "                tokens[-1].append(token)\n",
    "                probs[-1] = torch.cat((probs[-1], torch.tensor([prob])))\n",
    "                sense_indices[-1] = torch.cat((sense_indices[-1], torch.tensor([sense_index])))\n",
    "            else:\n",
    "                tokens.append([token])\n",
    "                probs.append(torch.tensor([prob]))\n",
    "                sense_indices.append(torch.tensor([sense_index]))\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    for i in range(len(probs)):\n",
    "        sample_prediction_index = torch.argmax(probs[i])\n",
    "        predictions_indices.append(sense_indices[i][sample_prediction_index])\n",
    "        predictions.append(vocab_itos[predictions_indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "['superior%1:18:02::',\n 'superior%1:18:02::',\n 'superior%1:18:02::',\n 'superior%1:18:01::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:18:00::',\n 'baggage%1:06:00::',\n 'baggage%1:06:00::']"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_indices = torch.stack(predictions_indices)\n",
    "predictions[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['superior%1:18:01::',\n 'superior%1:18:02::',\n 'superior%1:18:01::',\n 'superior%1:18:01::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:18:00::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:18:00::',\n 'baggage%1:06:00::',\n 'baggage%1:06:00::']"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if mode == \"WSDisambiguator\":\n",
    "    gold = [vocab_itos[x[\"sense_index\"]] for x in wic_dataset]\n",
    "    gold_indices = torch.tensor([x[\"sense_index\"] for x in wic_dataset])\n",
    "else:\n",
    "    # wsd_gold_keys_path = constants.VALID_SET_PATH.replace(\"../..\", \"..\") + constants.TXT_GOLD_KEYS_SUFFIX\n",
    "    wsd_gold_keys_path = constants.WIC_TEST_SET_WSD_KEYS_PATH.replace(\"../..\", \"..\")\n",
    "    gold = list(read_wsd_gold_keys(wsd_gold_keys_path).values())\n",
    "    gold_indices = torch.tensor([vocab[x] for x in gold])\n",
    "gold[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'0.6010'"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_acc = accuracy_score(predictions, gold)\n",
    "f\"{wsd_acc:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wic_samples = read_wic_corpus(constants.WIC_TEST_SET_PATH.replace('../..', '..'), constants.WIC_TEST_SET_WSD_KEYS_PATH.replace('../..', '..'))\n",
    "gold_wic = [x.sense1.sense_id == x.sense2.sense_id for x in wic_samples]\n",
    "\n",
    "predictions_wic = [predictions[i] == predictions[i+1] for i in range(0, len(predictions), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.6804'"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wic_acc = accuracy_score(predictions_wic, gold_wic)\n",
    "f\"{wic_acc:.4f}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _ = utils.plot_confusion_matrix(gold_wic, predictions_wic, normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "241c968aa5b2df334ebb21c90bd3ce900ec0c34e5bfa8ee6721f4d3c6b7b8eed"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nlp2021-hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
