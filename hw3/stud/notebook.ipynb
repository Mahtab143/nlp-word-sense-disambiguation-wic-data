{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word Sense Disambiguation (WSD) of Word-in-Context (WiC) data\n",
    "\n",
    "Third homework of the Natural Language Processing course 2021 @ Sapienza University of Rome.\n",
    "\n",
    "Prof. Roberto Navigli\n",
    "\n",
    "MSc in Computer Science\n",
    "\n",
    "**Author**: Andrea Gasparini - 1813486"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stud import constants, utils\n",
    "from stud.data import HParams, Pos, Token\n",
    "from stud.datasets import WSDDataset, GlossBERTDataset\n",
    "from stud.sense_inventories import build_senses_vocab, SenseInventory\n",
    "from stud.data_readers import read_wic_corpus, read_wsd_gold_keys, read_wsd_corpus\n",
    "from stud.pl_data_modules import WSDDataModule, GlossBERTDataModule\n",
    "from stud.transformer_embedder import TransformerEmbedder\n",
    "from stud.pl_modules import WordSenseDisambiguator, GlossBERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WSDisambiguator datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:04<00:00, 24.29it/s]\n",
      "100%|██████████| 1802/1802 [00:19<00:00, 92.70it/s]\n"
     ]
    }
   ],
   "source": [
    "training_corpus = read_wsd_corpus(f\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t\tf\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "semeval2007_corpus = read_wsd_corpus(f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t         \t\t\t f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "evaluation_corpus = read_wsd_corpus(f\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t\tf\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "\n",
    "wic_samples_dev = read_wic_corpus(constants.WIC_TEST_SET_PATH.replace('../..', '..'), constants.WIC_TEST_SET_WSD_KEYS_PATH.replace('../..', '..'))\n",
    "wic_corpus_dev = list()\n",
    "for wic_sample in wic_samples_dev:\n",
    "\twic_corpus_dev.append(wic_sample.sentence1)\n",
    "\twic_corpus_dev.append(wic_sample.sentence2)\n",
    "\n",
    "wic_samples_train = read_wic_corpus(constants.WIC_TRAIN_SET_PATH.replace('../..', '..'))\n",
    "wic_corpus_train = list()\n",
    "for wic_sample in wic_samples_train:\n",
    "\twic_corpus_train.append(wic_sample.sentence1)\n",
    "\twic_corpus_train.append(wic_sample.sentence2)\n",
    "\n",
    "sense_invent = SenseInventory(constants.GLOSSES_PATH.replace('../..', '..'), constants.LEMMA_POS_DICT_PATH.replace('../..', '..'))\n",
    "senses_vocabulary = build_senses_vocab(training_corpus + evaluation_corpus + wic_corpus_dev)\n",
    "\n",
    "embedder_model = utils.get_pretrained_model(constants.TRANSFORMER_EMBEDDER_PATH.replace('../..', '..'))\n",
    "\n",
    "train_set = WSDDataset(training_corpus, embedder_model, sense_invent, senses_vocabulary)\n",
    "torch.save(train_set, constants.PREPROCESSED_TRAIN_PATH.replace('../..', '..'))\n",
    "\n",
    "valid_set = WSDDataset(semeval2007_corpus, embedder_model, sense_invent, senses_vocabulary)\n",
    "torch.save(valid_set, constants.PREPROCESSED_VALID_PATH.replace('../..', '..'))\n",
    "\n",
    "test_set = WSDDataset(wic_corpus_dev, embedder_model, sense_invent, senses_vocabulary)\n",
    "torch.save(test_set, constants.PREPROCESSED_TEST_PATH.replace('../..', '..'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GlossBERT datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18149/18149 [00:39<00:00, 456.75it/s]\n"
     ]
    }
   ],
   "source": [
    "training_corpus = read_wsd_corpus(f\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t  f\"{constants.TRAIN_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "semeval2007_corpus = read_wsd_corpus(f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t         \t\t\t f\"{constants.VALID_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "evaluation_corpus = read_wsd_corpus(f\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.XML_DATA_SUFFIX.replace('../..', '..')}\",\n",
    "\t\t\t\t\t\t\t\t\tf\"{constants.TEST_SET_PATH.replace('../..', '..')}{constants.TXT_GOLD_KEYS_SUFFIX.replace('../..', '..')}\")\n",
    "\n",
    "wic_samples_dev = read_wic_corpus(constants.WIC_TEST_SET_PATH.replace('../..', '..'), constants.WIC_TEST_SET_WSD_KEYS_PATH.replace('../..', '..'))\n",
    "wic_corpus_dev = list()\n",
    "for wic_sample in wic_samples_dev:\n",
    "\twic_corpus_dev.append(wic_sample.sentence1)\n",
    "\twic_corpus_dev.append(wic_sample.sentence2)\n",
    "\n",
    "sense_invent = SenseInventory(constants.GLOSSES_PATH.replace('../..', '..'), constants.LEMMA_POS_DICT_PATH.replace('../..', '..'))\n",
    "\n",
    "SemCor_fraction_size = 0.5\n",
    "SemCor_fraction = random.sample(training_corpus, int(len(training_corpus) * SemCor_fraction_size))\n",
    "\n",
    "train_set = GlossBERTDataset.from_tokens(SemCor_fraction, sense_invent)\n",
    "train_set.save_as_json(f\"../data/preprocessed/GlossBERT/SemCor{int(SemCor_fraction_size * 100)}.json\")\n",
    "\n",
    "valid_set = GlossBERTDataset.from_tokens(semeval2007_corpus, sense_invent)\n",
    "valid_set.save_as_json(constants.PREPROCESSED_GLOSSBERT_VALID_PATH.replace('../..', '..'), indent=4)\n",
    "\n",
    "test_set = GlossBERTDataset.from_tokens(wic_corpus_dev, sense_invent)\n",
    "test_set.save_as_json(constants.PREPROCESSED_GLOSSBERT_TEST_PATH.replace('../..', '..'), indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WSDisambiguator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mandreagasparini\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/hw3/wandb/run-20220612_174029-1ubgvpko</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/andreagasparini/nlp_hw3/runs/1ubgvpko\" target=\"_blank\">WSDisambiguator_full_vocab</a></strong> to <a href=\"https://wandb.ai/andreagasparini/nlp_hw3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | senses_vocab  | Vocab            | 0     \n",
      "1 | model         | Sequential       | 15.8 M\n",
      "2 | loss_function | CrossEntropyLoss | 0     \n",
      "3 | train_acc     | Accuracy         | 0     \n",
      "4 | train_f1      | F1Score          | 0     \n",
      "5 | valid_acc     | Accuracy         | 0     \n",
      "6 | valid_f1      | F1Score          | 0     \n",
      "---------------------------------------------------\n",
      "15.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.022    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "143d8db97040498f9db26173e40366dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c050076982240388f71aa81b9f13b66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "utils.nltk_downloads()\n",
    "\n",
    "sense_inventory = SenseInventory(constants.GLOSSES_PATH.replace(\"../..\", \"..\"), constants.LEMMA_POS_DICT_PATH.replace(\"../..\", \"..\"))\n",
    "\n",
    "data_module = WSDDataModule(constants.PREPROCESSED_TRAIN_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\tconstants.PREPROCESSED_VALID_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\tconstants.PREPROCESSED_TEST_PATH.replace(\"../..\", \"..\"),\n",
    "                            utils.get_pretrained_model(constants.TRANSFORMER_EMBEDDER_PATH.replace(\"../..\", \"..\")),\n",
    "\t\t\t\t\t\t\tsense_inventory,\n",
    "\t\t\t\t\t\t\ttorch.load(\"../model/vocabularies/senses_vocabulary_train.pt\"),\n",
    "\t\t\t\t\t\t\tbatch_size=16,\n",
    "\t\t\t\t\t\t\tnum_workers=4,\n",
    "\t\t\t\t\t\t\tpin_memory=True)\n",
    "\n",
    "hparams = HParams(num_classes=len(data_module.senses_vocab),\n",
    "                  input_size=constants.TRANSFORMER_EMBEDDER_DIMENSION,\n",
    "                  batch_size=data_module.batch_size)\n",
    "\n",
    "MODELS_DIR = \"../model/\"\n",
    "MODEL_NAME = \"WSDisambiguator\"\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor=\"valid_wsd_accuracy\",\n",
    "                                            patience=5,\n",
    "                                            verbose=True,\n",
    "                                            mode=\"max\")\n",
    "\n",
    "check_point_callback = pl.callbacks.ModelCheckpoint(monitor=\"valid_wsd_accuracy\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_top_k=2,\n",
    "                                                    save_last=False,\n",
    "                                                    mode=\"max\",\n",
    "                                                    dirpath=MODELS_DIR,\n",
    "                                                    filename=MODEL_NAME + \"-{epoch}-{valid_wsd_loss:.4f}-\"\n",
    "                                                                          \"{valid_wsd_f1:.3f}-{valid_wsd_accuracy:.3f}\")\n",
    "\n",
    "wandb_logger = WandbLogger(offline=False, project=\"nlp_hw3\", name=MODEL_NAME)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    logger=wandb_logger,\n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=100,\n",
    "    callbacks=[early_stopping, check_point_callback],\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "model = WordSenseDisambiguator(hparams.as_dict(), data_module.senses_vocab, data_module.senses_vocab[constants.UNK_TOKEN])\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GlossBERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mandreagasparini\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.12.18 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.17"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/hw3/wandb/run-20220615_002822-2oofd11s</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/andreagasparini/nlp_hw3/runs/2oofd11s\" target=\"_blank\">GlossDistilBERT</a></strong> to <a href=\"https://wandb.ai/andreagasparini/nlp_hw3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/andrea/Projects/uni/magistrale/natural-language-processing/nlp2021-hw3/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                | Params\n",
      "------------------------------------------------------------\n",
      "0 | bert                | TransformerEmbedder | 65.2 M\n",
      "1 | classification_head | Linear              | 769   \n",
      "2 | loss_function       | BCELoss             | 0     \n",
      "3 | train_acc           | Accuracy            | 0     \n",
      "4 | train_f1            | F1Score             | 0     \n",
      "5 | valid_acc           | Accuracy            | 0     \n",
      "6 | valid_f1            | F1Score             | 0     \n",
      "------------------------------------------------------------\n",
      "65.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "65.2 M    Total params\n",
      "260.767   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5314ffb7ce584bef96e2f3b1a9714261"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "/home/andrea/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa9730d8a83a420b84103b7e29d2fef0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 1.95 GiB total capacity; 998.02 MiB already allocated; 80.31 MiB free; 1.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_24836/2908102588.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mGlossBERT\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"distilbert-base-cased\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatamodule\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_module\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[0mwandb\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinish\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    767\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    768\u001B[0m         self._call_and_handle_interrupt(\n\u001B[0;32m--> 769\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_impl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatamodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    770\u001B[0m         )\n\u001B[1;32m    771\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    719\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlauncher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlaunch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainer_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 721\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mtrainer_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    722\u001B[0m         \u001B[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    723\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mexception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[0mckpt_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_provided\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_connected\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    808\u001B[0m         )\n\u001B[0;32m--> 809\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mckpt_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    811\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1232\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_checkpoint_connector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresume_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1233\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1234\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1235\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1236\u001B[0m         \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetail\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1319\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredicting\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1322\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1323\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_pre_training_routine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1349\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1350\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_detect_anomaly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_detect_anomaly\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1351\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1352\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1353\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_run_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0m_EVALUATE_OUTPUT\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    266\u001B[0m         )\n\u001B[1;32m    267\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_training_epoch\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 268\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepoch_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_fetcher\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    269\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    270\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_training_batch\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 208\u001B[0;31m                 \u001B[0mbatch_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_progress\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mincrement_processed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautomatic_optimization\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m             \u001B[0moptimizers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_active_optimizers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_frequencies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msplit_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanual_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msplit_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, batch, *args, **kwargs)\u001B[0m\n\u001B[1;32m    205\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_batch_idx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_optimizers\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptim_progress\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_position\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 207\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_idx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    208\u001B[0m         )\n\u001B[1;32m    209\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36m_run_optimization\u001B[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001B[0m\n\u001B[1;32m    254\u001B[0m         \u001B[0;31m# gradient update with accumulated gradients\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    255\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 256\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_optimizer_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopt_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosure\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    257\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    258\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclosure\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconsume_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36m_optimizer_step\u001B[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    376\u001B[0m             \u001B[0mon_tpu\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccelerator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTPUAccelerator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m             \u001B[0musing_native_amp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mamp_backend\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mAMPType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNATIVE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 378\u001B[0;31m             \u001B[0musing_lbfgs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mis_lbfgs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    379\u001B[0m         )\n\u001B[1;32m    380\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_call_lightning_module_hook\u001B[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1591\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1592\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1593\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1594\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1595\u001B[0m         \u001B[0;31m# restore current_fx when nested context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\u001B[0m in \u001B[0;36moptimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001B[0m\n\u001B[1;32m   1642\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1643\u001B[0m         \"\"\"\n\u001B[0;32m-> 1644\u001B[0;31m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclosure\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptimizer_closure\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1645\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1646\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0moptimizer_zero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer_idx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/core/optimizer.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_strategy\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0mstep_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_strategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_optimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_optimizer_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosure\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_on_after_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\u001B[0m in \u001B[0;36moptimizer_step\u001B[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    191\u001B[0m         \"\"\"\n\u001B[1;32m    192\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 193\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecision_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopt_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosure\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    194\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_setup_model_and_optimizers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mModule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mOptimizer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mModule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mOptimizer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\u001B[0m in \u001B[0;36moptimizer_step\u001B[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLightningModule\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m             \u001B[0mclosure\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpartial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_closure\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosure\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclosure\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mclosure\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    156\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    157\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_track_grad_norm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"pl.Trainer\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/torch/optim/adam.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    151\u001B[0m                    \u001B[0mweight_decay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgroup\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'weight_decay'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m                    \u001B[0meps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgroup\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'eps'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 153\u001B[0;31m                    maximize=group['maximize'])\n\u001B[0m\u001B[1;32m    154\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/nlp2021-hw3/lib/python3.7/site-packages/torch/optim/_functional.py\u001B[0m in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    103\u001B[0m             \u001B[0mdenom\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmax_exp_avg_sqs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias_correction2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 105\u001B[0;31m             \u001B[0mdenom\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mexp_avg_sq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias_correction2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    106\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 1.95 GiB total capacity; 998.02 MiB already allocated; 80.31 MiB free; 1.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "utils.nltk_downloads()\n",
    "\n",
    "sense_inventory = SenseInventory(constants.GLOSSES_PATH.replace('../..', '..'), constants.LEMMA_POS_DICT_PATH.replace('../..', '..'))\n",
    "\n",
    "data_module = GlossBERTDataModule(constants.PREPROCESSED_GLOSSBERT_TRAIN_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\t      constants.PREPROCESSED_GLOSSBERT_VALID_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\t      constants.PREPROCESSED_GLOSSBERT_TEST_PATH.replace(\"../..\", \"..\"),\n",
    "\t\t\t\t\t\t\t      batch_size=8,\n",
    "\t\t\t\t\t\t\t      num_workers=4,\n",
    "\t\t\t\t\t\t\t      pin_memory=True)\n",
    "\n",
    "hparams = HParams(num_classes=1, learning_rate=2e-5, batch_size=data_module.batch_size, input_size=None)\n",
    "\n",
    "MODELS_DIR = \"../model/\"\n",
    "MODEL_NAME = \"GlossBERT\"\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor=\"valid_wsd_accuracy\",\n",
    "                                            patience=5,\n",
    "                                            verbose=True,\n",
    "                                            mode=\"max\")\n",
    "\n",
    "check_point_callback = pl.callbacks.ModelCheckpoint(monitor=\"valid_wsd_accuracy\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_top_k=4,\n",
    "                                                    save_last=False,\n",
    "                                                    mode=\"max\",\n",
    "                                                    dirpath=MODELS_DIR,\n",
    "                                                    filename=MODEL_NAME + \"-{epoch}-{valid_wsd_loss:.4f}-\"\n",
    "                                                                          \"{valid_wsd_f1:.3f}-{valid_wsd_accuracy:.3f}\")\n",
    "\n",
    "wandb_logger = WandbLogger(offline=False, project=\"nlp_hw3\", name=MODEL_NAME)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    logger=wandb_logger,\n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=10,\n",
    "    callbacks=[early_stopping, check_point_callback],\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "model = GlossBERT(hparams.as_dict(), constants.TRANSFORMER_EMBEDDER_PATH.replace('../..', '..'))\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mode = \"GlossBERT\" # \"WSDisambiguator\" | \"GlossBERT\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "bert_model_name = utils.get_pretrained_model(constants.TRANSFORMER_EMBEDDER_PATH)\n",
    "\n",
    "if mode == \"WSDisambiguator\":\n",
    "    vocab = torch.load(\"../model/vocabularies/senses_vocabulary_train.pt\")\n",
    "    wic_dataset = WSDDataset.from_preprocessed(constants.PREPROCESSED_TEST_PATH.replace('../..', '..'))\n",
    "    wic_dataset.to_device(model.device)\n",
    "else:\n",
    "    vocab = torch.load(\"../model/vocabularies/senses_vocabulary_full.pt\")\n",
    "    # wic_dataset = GlossBERTDataset.from_json(constants.PREPROCESSED_GLOSSBERT_VALID_PATH.replace('../..', '..'))\n",
    "    wic_dataset = GlossBERTDataset.from_json(constants.PREPROCESSED_GLOSSBERT_TEST_PATH.replace('../..', '..'))\n",
    "    # wic_dataset = GlossBERTDataset.from_json(\"../data/preprocessed/GlossBERT/wic_dev_no_pos.json\")\n",
    "\n",
    "vocab_itos = vocab.get_itos()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "if mode == \"WSDisambiguator\":\n",
    "    MODEL_PATH = \"../model/WSDisambiguator.ckpt\"\n",
    "    model = WordSenseDisambiguator.load_from_checkpoint(MODEL_PATH, senses_vocab=vocab)\n",
    "else:\n",
    "    MODEL_PATH = \"../model/GlossBERT.ckpt\"\n",
    "    model = GlossBERT.load_from_checkpoint(MODEL_PATH, bert_model_name_or_path=\"../model/bert-base-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8622 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:   0%|          | 1/8622 [00:00<1:38:47,  1.45it/s]\u001B[A\n",
      "Evaluating:   0%|          | 9/8622 [00:01<27:47,  5.16it/s]  \u001B[A\n",
      "Evaluating:   0%|          | 17/8622 [00:02<17:58,  7.98it/s]\u001B[A\n",
      "Evaluating:   0%|          | 25/8622 [00:02<12:13, 11.71it/s]\u001B[A\n",
      "Evaluating:   0%|          | 33/8622 [00:03<09:25, 15.19it/s]\u001B[A\n",
      "Evaluating:   0%|          | 41/8622 [00:03<07:58, 17.92it/s]\u001B[A\n",
      "Evaluating:   1%|          | 49/8622 [00:03<07:50, 18.23it/s]\u001B[A\n",
      "Evaluating:   1%|          | 57/8622 [00:04<09:50, 14.50it/s]\u001B[A\n",
      "Evaluating:   1%|          | 65/8622 [00:05<09:39, 14.78it/s]\u001B[A\n",
      "Evaluating:   1%|          | 73/8622 [00:05<08:38, 16.49it/s]\u001B[A\n",
      "Evaluating:   1%|          | 81/8622 [00:05<08:05, 17.58it/s]\u001B[A\n",
      "Evaluating:   1%|          | 89/8622 [00:06<09:29, 14.99it/s]\u001B[A\n",
      "Evaluating:   1%|          | 97/8622 [00:07<10:48, 13.15it/s]\u001B[A\n",
      "Evaluating:   1%|          | 105/8622 [00:08<12:23, 11.45it/s]\u001B[A\n",
      "Evaluating:   1%|▏         | 113/8622 [00:09<14:35,  9.72it/s]\u001B[A\n",
      "Evaluating:   1%|▏         | 121/8622 [00:09<12:02, 11.76it/s]\u001B[A\n",
      "Evaluating:   1%|▏         | 129/8622 [00:10<10:51, 13.04it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 137/8622 [00:10<09:38, 14.67it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 145/8622 [00:10<08:11, 17.23it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 153/8622 [00:11<08:02, 17.56it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 161/8622 [00:11<07:21, 19.16it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 169/8622 [00:11<06:43, 20.93it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 177/8622 [00:12<06:18, 22.33it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 185/8622 [00:12<06:08, 22.87it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 193/8622 [00:12<05:43, 24.56it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 201/8622 [00:12<05:10, 27.14it/s]\u001B[A\n",
      "Evaluating:   2%|▏         | 209/8622 [00:13<04:59, 28.13it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 217/8622 [00:14<09:50, 14.22it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 225/8622 [00:15<10:29, 13.35it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 233/8622 [00:15<11:17, 12.38it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 241/8622 [00:16<12:35, 11.09it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 249/8622 [00:17<11:34, 12.06it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 257/8622 [00:17<10:20, 13.48it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 265/8622 [00:18<10:44, 12.98it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 273/8622 [00:18<09:19, 14.93it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 281/8622 [00:19<07:53, 17.62it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 289/8622 [00:19<06:52, 20.20it/s]\u001B[A\n",
      "Evaluating:   3%|▎         | 297/8622 [00:19<06:33, 21.13it/s]\u001B[A\n",
      "Evaluating:   4%|▎         | 305/8622 [00:20<09:02, 15.33it/s]\u001B[A\n",
      "Evaluating:   4%|▎         | 313/8622 [00:21<09:47, 14.13it/s]\u001B[A\n",
      "Evaluating:   4%|▎         | 321/8622 [00:21<10:08, 13.65it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 329/8622 [00:22<09:27, 14.60it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 337/8622 [00:22<09:03, 15.23it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 345/8622 [00:23<08:07, 16.96it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 353/8622 [00:23<07:16, 18.92it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 361/8622 [00:23<07:53, 17.43it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 369/8622 [00:24<08:59, 15.29it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 377/8622 [00:24<07:42, 17.83it/s]\u001B[A\n",
      "Evaluating:   4%|▍         | 385/8622 [00:25<08:59, 15.28it/s]\u001B[A\n",
      "Evaluating:   5%|▍         | 393/8622 [00:25<08:24, 16.30it/s]\u001B[A\n",
      "Evaluating:   5%|▍         | 401/8622 [00:26<08:12, 16.70it/s]\u001B[A\n",
      "Evaluating:   5%|▍         | 409/8622 [00:26<08:27, 16.19it/s]\u001B[A\n",
      "Evaluating:   5%|▍         | 417/8622 [00:27<08:01, 17.04it/s]\u001B[A\n",
      "Evaluating:   5%|▍         | 425/8622 [00:27<07:51, 17.37it/s]\u001B[A\n",
      "Evaluating:   5%|▌         | 433/8622 [00:28<07:45, 17.59it/s]\u001B[A\n",
      "Evaluating:   5%|▌         | 441/8622 [00:29<09:19, 14.63it/s]\u001B[A\n",
      "Evaluating:   5%|▌         | 449/8622 [00:30<11:46, 11.56it/s]\u001B[A\n",
      "Evaluating:   5%|▌         | 457/8622 [00:30<10:32, 12.90it/s]\u001B[A\n",
      "Evaluating:   5%|▌         | 465/8622 [00:30<09:46, 13.91it/s]\u001B[A\n",
      "Evaluating:   5%|▌         | 473/8622 [00:31<08:36, 15.78it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 481/8622 [00:31<07:45, 17.49it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 489/8622 [00:32<08:03, 16.81it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 497/8622 [00:32<07:04, 19.16it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 505/8622 [00:33<08:16, 16.35it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 513/8622 [00:33<09:11, 14.71it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 521/8622 [00:34<08:07, 16.63it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 529/8622 [00:34<07:28, 18.06it/s]\u001B[A\n",
      "Evaluating:   6%|▌         | 537/8622 [00:35<07:52, 17.10it/s]\u001B[A\n",
      "Evaluating:   6%|▋         | 545/8622 [00:35<07:00, 19.21it/s]\u001B[A\n",
      "Evaluating:   6%|▋         | 553/8622 [00:36<09:15, 14.51it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 561/8622 [00:36<09:29, 14.16it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 569/8622 [00:37<09:50, 13.65it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 577/8622 [00:37<09:45, 13.75it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 585/8622 [00:38<11:40, 11.48it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 593/8622 [00:39<13:21, 10.01it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 601/8622 [00:40<13:17, 10.06it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 609/8622 [00:42<16:17,  8.19it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 617/8622 [00:43<17:19,  7.70it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 625/8622 [00:45<22:34,  5.91it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 633/8622 [00:46<18:57,  7.02it/s]\u001B[A\n",
      "Evaluating:   7%|▋         | 641/8622 [00:46<16:32,  8.04it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 649/8622 [00:47<13:41,  9.71it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 657/8622 [00:47<11:09, 11.90it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 665/8622 [00:47<09:08, 14.49it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 673/8622 [00:48<07:59, 16.58it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 681/8622 [00:48<07:45, 17.06it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 689/8622 [00:49<08:23, 15.75it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 697/8622 [00:49<07:58, 16.57it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 705/8622 [00:49<06:52, 19.20it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 713/8622 [00:50<06:15, 21.07it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 721/8622 [00:50<07:32, 17.46it/s]\u001B[A\n",
      "Evaluating:   8%|▊         | 729/8622 [00:51<09:08, 14.40it/s]\u001B[A\n",
      "Evaluating:   9%|▊         | 737/8622 [00:52<10:01, 13.11it/s]\u001B[A\n",
      "Evaluating:   9%|▊         | 745/8622 [00:52<08:50, 14.84it/s]\u001B[A\n",
      "Evaluating:   9%|▊         | 753/8622 [00:53<08:06, 16.17it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 761/8622 [00:53<09:29, 13.82it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 769/8622 [00:54<08:42, 15.04it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 777/8622 [00:54<08:03, 16.21it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 785/8622 [00:55<09:07, 14.33it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 793/8622 [00:55<08:24, 15.51it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 801/8622 [00:56<07:52, 16.54it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 809/8622 [00:56<07:38, 17.05it/s]\u001B[A\n",
      "Evaluating:   9%|▉         | 817/8622 [00:56<07:00, 18.57it/s]\u001B[A\n",
      "Evaluating:  10%|▉         | 825/8622 [00:57<06:35, 19.71it/s]\u001B[A\n",
      "Evaluating:  10%|▉         | 833/8622 [00:57<05:49, 22.27it/s]\u001B[A\n",
      "Evaluating:  10%|▉         | 841/8622 [00:57<05:22, 24.16it/s]\u001B[A\n",
      "Evaluating:  10%|▉         | 849/8622 [00:58<05:37, 23.06it/s]\u001B[A\n",
      "Evaluating:  10%|▉         | 857/8622 [00:58<05:36, 23.08it/s]\u001B[A\n",
      "Evaluating:  10%|█         | 865/8622 [00:58<05:39, 22.85it/s]\u001B[A\n",
      "Evaluating:  10%|█         | 873/8622 [00:59<06:21, 20.31it/s]\u001B[A\n",
      "Evaluating:  10%|█         | 881/8622 [00:59<06:16, 20.56it/s]\u001B[A\n",
      "Evaluating:  10%|█         | 889/8622 [01:00<07:32, 17.10it/s]\u001B[A\n",
      "Evaluating:  10%|█         | 897/8622 [01:01<09:06, 14.13it/s]\u001B[A\n",
      "Evaluating:  10%|█         | 905/8622 [01:01<09:13, 13.94it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 913/8622 [01:02<09:00, 14.26it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 921/8622 [01:02<08:00, 16.01it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 929/8622 [01:03<07:16, 17.63it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 937/8622 [01:03<07:24, 17.28it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 945/8622 [01:03<07:22, 17.35it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 953/8622 [01:04<07:22, 17.33it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 961/8622 [01:04<06:27, 19.78it/s]\u001B[A\n",
      "Evaluating:  11%|█         | 969/8622 [01:05<06:20, 20.13it/s]\u001B[A\n",
      "Evaluating:  11%|█▏        | 977/8622 [01:05<06:43, 18.96it/s]\u001B[A\n",
      "Evaluating:  11%|█▏        | 985/8622 [01:06<07:18, 17.40it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 993/8622 [01:06<07:26, 17.10it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1001/8622 [01:07<07:35, 16.72it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1009/8622 [01:07<08:14, 15.40it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1017/8622 [01:08<07:34, 16.73it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1025/8622 [01:08<08:02, 15.76it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1033/8622 [01:09<08:16, 15.29it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1041/8622 [01:09<09:19, 13.56it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1049/8622 [01:10<08:43, 14.47it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1057/8622 [01:11<08:52, 14.21it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1065/8622 [01:11<09:02, 13.93it/s]\u001B[A\n",
      "Evaluating:  12%|█▏        | 1073/8622 [01:12<09:16, 13.57it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1081/8622 [01:12<09:08, 13.75it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1089/8622 [01:13<09:34, 13.11it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1097/8622 [01:14<09:23, 13.36it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1105/8622 [01:14<10:13, 12.24it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1113/8622 [01:15<08:59, 13.93it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1121/8622 [01:15<08:21, 14.96it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1129/8622 [01:16<08:09, 15.30it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1137/8622 [01:16<07:40, 16.25it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1145/8622 [01:17<07:13, 17.23it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1153/8622 [01:17<06:58, 17.84it/s]\u001B[A\n",
      "Evaluating:  13%|█▎        | 1161/8622 [01:17<06:55, 17.97it/s]\u001B[A\n",
      "Evaluating:  14%|█▎        | 1169/8622 [01:18<07:56, 15.63it/s]\u001B[A\n",
      "Evaluating:  14%|█▎        | 1177/8622 [01:19<08:41, 14.29it/s]\u001B[A\n",
      "Evaluating:  14%|█▎        | 1185/8622 [01:19<08:38, 14.35it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1193/8622 [01:20<08:15, 14.99it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1201/8622 [01:20<08:32, 14.47it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1209/8622 [01:21<09:29, 13.01it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1217/8622 [01:22<08:56, 13.79it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1225/8622 [01:22<07:48, 15.80it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1233/8622 [01:22<07:18, 16.86it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1241/8622 [01:23<08:34, 14.35it/s]\u001B[A\n",
      "Evaluating:  14%|█▍        | 1249/8622 [01:24<08:12, 14.97it/s]\u001B[A\n",
      "Evaluating:  15%|█▍        | 1257/8622 [01:24<08:15, 14.87it/s]\u001B[A\n",
      "Evaluating:  15%|█▍        | 1265/8622 [01:25<07:50, 15.63it/s]\u001B[A\n",
      "Evaluating:  15%|█▍        | 1273/8622 [01:25<07:48, 15.69it/s]\u001B[A\n",
      "Evaluating:  15%|█▍        | 1281/8622 [01:26<07:46, 15.75it/s]\u001B[A\n",
      "Evaluating:  15%|█▍        | 1289/8622 [01:26<08:57, 13.63it/s]\u001B[A\n",
      "Evaluating:  15%|█▌        | 1297/8622 [01:27<08:53, 13.73it/s]\u001B[A\n",
      "Evaluating:  15%|█▌        | 1305/8622 [01:28<09:08, 13.34it/s]\u001B[A\n",
      "Evaluating:  15%|█▌        | 1313/8622 [01:29<12:19,  9.88it/s]\u001B[A\n",
      "Evaluating:  15%|█▌        | 1321/8622 [01:30<14:09,  8.60it/s]\u001B[A\n",
      "Evaluating:  15%|█▌        | 1329/8622 [01:32<20:00,  6.08it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1337/8622 [01:33<15:20,  7.92it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1345/8622 [01:33<12:11,  9.95it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1353/8622 [01:33<09:23, 12.89it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1361/8622 [01:34<08:32, 14.16it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1369/8622 [01:34<07:56, 15.23it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1377/8622 [01:34<07:10, 16.81it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1385/8622 [01:35<06:19, 19.06it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1393/8622 [01:35<05:56, 20.26it/s]\u001B[A\n",
      "Evaluating:  16%|█▌        | 1401/8622 [01:35<05:45, 20.90it/s]\u001B[A\n",
      "Evaluating:  16%|█▋        | 1409/8622 [01:36<05:38, 21.28it/s]\u001B[A\n",
      "Evaluating:  16%|█▋        | 1417/8622 [01:37<08:27, 14.20it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1425/8622 [01:38<09:58, 12.02it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1433/8622 [01:39<12:42,  9.43it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1441/8622 [01:40<13:50,  8.65it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1449/8622 [01:41<14:49,  8.07it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1457/8622 [01:42<14:34,  8.19it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1465/8622 [01:42<12:14,  9.74it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1473/8622 [01:43<11:04, 10.76it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1481/8622 [01:44<10:23, 11.45it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1489/8622 [01:44<09:36, 12.37it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1497/8622 [01:45<11:24, 10.41it/s]\u001B[A\n",
      "Evaluating:  17%|█▋        | 1505/8622 [01:46<10:19, 11.48it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1513/8622 [01:46<08:56, 13.25it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1521/8622 [01:47<07:59, 14.82it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1529/8622 [01:47<07:17, 16.21it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1537/8622 [01:47<07:11, 16.41it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1545/8622 [01:48<08:36, 13.71it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1553/8622 [01:49<11:40, 10.10it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1561/8622 [01:51<14:03,  8.37it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1569/8622 [01:52<13:17,  8.84it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1577/8622 [01:52<12:28,  9.41it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1585/8622 [01:53<13:40,  8.58it/s]\u001B[A\n",
      "Evaluating:  18%|█▊        | 1593/8622 [01:54<13:58,  8.38it/s]\u001B[A\n",
      "Evaluating:  19%|█▊        | 1601/8622 [01:55<12:36,  9.28it/s]\u001B[A\n",
      "Evaluating:  19%|█▊        | 1609/8622 [01:56<10:40, 10.96it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1617/8622 [01:56<09:52, 11.82it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1625/8622 [01:57<09:02, 12.91it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1633/8622 [01:57<08:05, 14.39it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1641/8622 [01:58<09:21, 12.44it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1649/8622 [01:58<09:01, 12.88it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1657/8622 [01:59<08:16, 14.04it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1665/8622 [01:59<07:54, 14.67it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1673/8622 [02:00<07:37, 15.19it/s]\u001B[A\n",
      "Evaluating:  19%|█▉        | 1681/8622 [02:00<07:06, 16.27it/s]\u001B[A\n",
      "Evaluating:  20%|█▉        | 1689/8622 [02:01<06:52, 16.80it/s]\u001B[A\n",
      "Evaluating:  20%|█▉        | 1697/8622 [02:01<08:19, 13.86it/s]\u001B[A\n",
      "Evaluating:  20%|█▉        | 1705/8622 [02:02<08:43, 13.20it/s]\u001B[A\n",
      "Evaluating:  20%|█▉        | 1713/8622 [02:02<07:24, 15.53it/s]\u001B[A\n",
      "Evaluating:  20%|█▉        | 1721/8622 [02:03<06:51, 16.76it/s]\u001B[A\n",
      "Evaluating:  20%|██        | 1729/8622 [02:03<06:51, 16.74it/s]\u001B[A\n",
      "Evaluating:  20%|██        | 1737/8622 [02:04<07:08, 16.06it/s]\u001B[A\n",
      "Evaluating:  20%|██        | 1745/8622 [02:04<07:26, 15.39it/s]\u001B[A\n",
      "Evaluating:  20%|██        | 1753/8622 [02:05<07:39, 14.94it/s]\u001B[A\n",
      "Evaluating:  20%|██        | 1761/8622 [02:06<08:22, 13.65it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1769/8622 [02:06<08:16, 13.79it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1777/8622 [02:07<08:19, 13.69it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1785/8622 [02:08<08:48, 12.94it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1793/8622 [02:08<08:47, 12.95it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1801/8622 [02:09<08:36, 13.22it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1809/8622 [02:09<08:27, 13.43it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1817/8622 [02:10<08:38, 13.12it/s]\u001B[A\n",
      "Evaluating:  21%|██        | 1825/8622 [02:11<08:27, 13.38it/s]\u001B[A\n",
      "Evaluating:  21%|██▏       | 1833/8622 [02:12<11:31,  9.82it/s]\u001B[A\n",
      "Evaluating:  21%|██▏       | 1841/8622 [02:13<11:45,  9.61it/s]\u001B[A\n",
      "Evaluating:  21%|██▏       | 1849/8622 [02:14<11:32,  9.79it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1857/8622 [02:14<10:33, 10.68it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1865/8622 [02:15<11:43,  9.60it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1873/8622 [02:16<12:15,  9.18it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1881/8622 [02:17<10:26, 10.76it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1889/8622 [02:17<08:22, 13.41it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1897/8622 [02:17<07:22, 15.19it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1905/8622 [02:18<06:41, 16.73it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1913/8622 [02:18<06:59, 16.01it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1921/8622 [02:19<07:28, 14.94it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1929/8622 [02:19<06:33, 17.00it/s]\u001B[A\n",
      "Evaluating:  22%|██▏       | 1937/8622 [02:19<06:16, 17.73it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 1945/8622 [02:20<05:57, 18.68it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 1953/8622 [02:20<05:48, 19.13it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 1961/8622 [02:21<06:24, 17.32it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 1969/8622 [02:21<05:51, 18.94it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 1977/8622 [02:21<05:51, 18.89it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 1985/8622 [02:22<06:04, 18.20it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 1993/8622 [02:22<06:25, 17.18it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 2001/8622 [02:23<06:09, 17.91it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 2009/8622 [02:23<06:22, 17.27it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 2017/8622 [02:24<07:08, 15.42it/s]\u001B[A\n",
      "Evaluating:  23%|██▎       | 2025/8622 [02:24<06:28, 16.97it/s]\u001B[A\n",
      "Evaluating:  24%|██▎       | 2033/8622 [02:25<07:00, 15.67it/s]\u001B[A\n",
      "Evaluating:  24%|██▎       | 2041/8622 [02:25<06:42, 16.34it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2049/8622 [02:26<07:19, 14.97it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2057/8622 [02:27<07:25, 14.75it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2065/8622 [02:27<07:15, 15.06it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2073/8622 [02:28<07:49, 13.96it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2081/8622 [02:28<07:43, 14.12it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2089/8622 [02:29<07:35, 14.36it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2097/8622 [02:30<11:39,  9.33it/s]\u001B[A\n",
      "Evaluating:  24%|██▍       | 2105/8622 [02:31<10:19, 10.53it/s]\u001B[A\n",
      "Evaluating:  25%|██▍       | 2113/8622 [02:31<09:06, 11.91it/s]\u001B[A\n",
      "Evaluating:  25%|██▍       | 2121/8622 [02:32<07:57, 13.63it/s]\u001B[A\n",
      "Evaluating:  25%|██▍       | 2129/8622 [02:32<07:08, 15.14it/s]\u001B[A\n",
      "Evaluating:  25%|██▍       | 2137/8622 [02:33<06:21, 17.01it/s]\u001B[A\n",
      "Evaluating:  25%|██▍       | 2145/8622 [02:33<05:48, 18.60it/s]\u001B[A\n",
      "Evaluating:  25%|██▍       | 2153/8622 [02:34<06:54, 15.61it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2161/8622 [02:35<09:58, 10.79it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2169/8622 [02:35<09:12, 11.68it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2177/8622 [02:36<08:11, 13.11it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2185/8622 [02:37<10:03, 10.67it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2193/8622 [02:38<12:03,  8.89it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2201/8622 [02:41<18:22,  5.82it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2209/8622 [02:41<14:38,  7.30it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2217/8622 [02:42<11:52,  8.99it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2225/8622 [02:42<11:56,  8.93it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2233/8622 [02:43<10:18, 10.34it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2241/8622 [02:43<09:20, 11.38it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2249/8622 [02:44<08:38, 12.29it/s]\u001B[A\n",
      "Evaluating:  26%|██▌       | 2257/8622 [02:45<08:31, 12.45it/s]\u001B[A\n",
      "Evaluating:  26%|██▋       | 2265/8622 [02:45<08:51, 11.97it/s]\u001B[A\n",
      "Evaluating:  26%|██▋       | 2273/8622 [02:46<09:29, 11.15it/s]\u001B[A\n",
      "Evaluating:  26%|██▋       | 2281/8622 [02:47<08:57, 11.80it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2289/8622 [02:48<09:23, 11.24it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2297/8622 [02:48<08:47, 11.98it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2305/8622 [02:49<08:43, 12.06it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2313/8622 [02:49<08:06, 12.97it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2321/8622 [02:50<07:20, 14.31it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2329/8622 [02:51<08:21, 12.54it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2337/8622 [02:51<09:09, 11.43it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2345/8622 [02:52<08:31, 12.27it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2353/8622 [02:53<08:32, 12.22it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2361/8622 [02:54<09:53, 10.56it/s]\u001B[A\n",
      "Evaluating:  27%|██▋       | 2369/8622 [02:55<10:39,  9.78it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2377/8622 [02:55<09:18, 11.18it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2385/8622 [02:56<08:47, 11.81it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2393/8622 [02:56<08:03, 12.89it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2401/8622 [02:58<13:01,  7.96it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2409/8622 [02:59<12:56,  8.01it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2417/8622 [03:00<12:04,  8.56it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2425/8622 [03:00<09:52, 10.47it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2433/8622 [03:01<09:17, 11.11it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2441/8622 [03:01<08:35, 11.98it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2449/8622 [03:02<07:22, 13.95it/s]\u001B[A\n",
      "Evaluating:  28%|██▊       | 2457/8622 [03:02<06:30, 15.78it/s]\u001B[A\n",
      "Evaluating:  29%|██▊       | 2465/8622 [03:02<05:40, 18.09it/s]\u001B[A\n",
      "Evaluating:  29%|██▊       | 2473/8622 [03:03<05:07, 20.00it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2481/8622 [03:03<04:40, 21.86it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2489/8622 [03:03<05:07, 19.92it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2497/8622 [03:04<06:16, 16.26it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2505/8622 [03:05<06:40, 15.26it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2513/8622 [03:05<06:28, 15.74it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2521/8622 [03:06<06:29, 15.68it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2529/8622 [03:06<06:44, 15.06it/s]\u001B[A\n",
      "Evaluating:  29%|██▉       | 2537/8622 [03:07<06:36, 15.36it/s]\u001B[A\n",
      "Evaluating:  30%|██▉       | 2545/8622 [03:07<07:16, 13.92it/s]\u001B[A\n",
      "Evaluating:  30%|██▉       | 2553/8622 [03:08<07:31, 13.45it/s]\u001B[A\n",
      "Evaluating:  30%|██▉       | 2561/8622 [03:09<07:02, 14.35it/s]\u001B[A\n",
      "Evaluating:  30%|██▉       | 2569/8622 [03:09<06:42, 15.05it/s]\u001B[A\n",
      "Evaluating:  30%|██▉       | 2577/8622 [03:09<06:23, 15.75it/s]\u001B[A\n",
      "Evaluating:  30%|██▉       | 2585/8622 [03:10<06:11, 16.27it/s]\u001B[A\n",
      "Evaluating:  30%|███       | 2593/8622 [03:10<05:50, 17.22it/s]\u001B[A\n",
      "Evaluating:  30%|███       | 2601/8622 [03:11<07:18, 13.73it/s]\u001B[A\n",
      "Evaluating:  30%|███       | 2609/8622 [03:12<08:48, 11.38it/s]\u001B[A\n",
      "Evaluating:  30%|███       | 2617/8622 [03:13<09:08, 10.96it/s]\u001B[A\n",
      "Evaluating:  30%|███       | 2625/8622 [03:14<09:07, 10.94it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2633/8622 [03:14<08:19, 11.99it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2641/8622 [03:15<08:03, 12.37it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2649/8622 [03:15<07:04, 14.08it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2657/8622 [03:16<06:46, 14.69it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2665/8622 [03:16<05:57, 16.69it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2673/8622 [03:17<06:07, 16.19it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2681/8622 [03:17<05:51, 16.88it/s]\u001B[A\n",
      "Evaluating:  31%|███       | 2689/8622 [03:17<05:41, 17.37it/s]\u001B[A\n",
      "Evaluating:  31%|███▏      | 2697/8622 [03:18<06:00, 16.42it/s]\u001B[A\n",
      "Evaluating:  31%|███▏      | 2705/8622 [03:18<06:12, 15.87it/s]\u001B[A\n",
      "Evaluating:  31%|███▏      | 2713/8622 [03:20<08:46, 11.22it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2721/8622 [03:20<07:11, 13.66it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2729/8622 [03:20<06:41, 14.67it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2737/8622 [03:21<06:17, 15.58it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2745/8622 [03:22<07:18, 13.39it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2753/8622 [03:23<09:36, 10.18it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2761/8622 [03:24<09:43, 10.04it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2769/8622 [03:25<10:47,  9.04it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2777/8622 [03:26<10:24,  9.36it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2785/8622 [03:26<09:57,  9.77it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2793/8622 [03:28<12:24,  7.83it/s]\u001B[A\n",
      "Evaluating:  32%|███▏      | 2801/8622 [03:31<18:42,  5.19it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2809/8622 [03:32<18:58,  5.10it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2817/8622 [03:33<15:43,  6.15it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2825/8622 [03:33<12:44,  7.58it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2833/8622 [03:34<10:33,  9.14it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2841/8622 [03:34<09:02, 10.66it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2849/8622 [03:35<08:00, 12.00it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2857/8622 [03:35<07:19, 13.12it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2865/8622 [03:36<06:23, 14.99it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2873/8622 [03:36<05:37, 17.05it/s]\u001B[A\n",
      "Evaluating:  33%|███▎      | 2881/8622 [03:36<05:43, 16.72it/s]\u001B[A\n",
      "Evaluating:  34%|███▎      | 2889/8622 [03:37<05:26, 17.57it/s]\u001B[A\n",
      "Evaluating:  34%|███▎      | 2897/8622 [03:37<06:10, 15.44it/s]\u001B[A\n",
      "Evaluating:  34%|███▎      | 2905/8622 [03:39<08:37, 11.04it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2913/8622 [03:40<11:41,  8.13it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2921/8622 [03:41<11:10,  8.50it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2929/8622 [03:43<14:44,  6.44it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2937/8622 [03:44<14:58,  6.33it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2945/8622 [03:45<13:09,  7.19it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2953/8622 [03:46<13:34,  6.96it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2961/8622 [03:47<12:09,  7.76it/s]\u001B[A\n",
      "Evaluating:  34%|███▍      | 2969/8622 [03:48<09:59,  9.43it/s]\u001B[A\n",
      "Evaluating:  35%|███▍      | 2977/8622 [03:48<08:39, 10.88it/s]\u001B[A\n",
      "Evaluating:  35%|███▍      | 2985/8622 [03:49<08:03, 11.65it/s]\u001B[A\n",
      "Evaluating:  35%|███▍      | 2993/8622 [03:49<07:20, 12.79it/s]\u001B[A\n",
      "Evaluating:  35%|███▍      | 3001/8622 [03:50<07:21, 12.73it/s]\u001B[A\n",
      "Evaluating:  35%|███▍      | 3009/8622 [03:50<06:31, 14.32it/s]\u001B[A\n",
      "Evaluating:  35%|███▍      | 3017/8622 [03:51<05:59, 15.58it/s]\u001B[A\n",
      "Evaluating:  35%|███▌      | 3025/8622 [03:51<06:21, 14.68it/s]\u001B[A\n",
      "Evaluating:  35%|███▌      | 3033/8622 [03:52<07:19, 12.73it/s]\u001B[A\n",
      "Evaluating:  35%|███▌      | 3041/8622 [03:52<06:59, 13.29it/s]\u001B[A\n",
      "Evaluating:  35%|███▌      | 3049/8622 [03:53<06:30, 14.26it/s]\u001B[A\n",
      "Evaluating:  35%|███▌      | 3057/8622 [03:53<06:16, 14.77it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3065/8622 [03:54<06:07, 15.13it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3073/8622 [03:55<06:19, 14.61it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3081/8622 [03:55<06:23, 14.45it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3089/8622 [03:56<06:06, 15.11it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3097/8622 [03:56<05:16, 17.46it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3105/8622 [03:56<05:33, 16.55it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3113/8622 [03:57<06:10, 14.86it/s]\u001B[A\n",
      "Evaluating:  36%|███▌      | 3121/8622 [03:57<05:34, 16.43it/s]\u001B[A\n",
      "Evaluating:  36%|███▋      | 3129/8622 [03:58<06:09, 14.87it/s]\u001B[A\n",
      "Evaluating:  36%|███▋      | 3137/8622 [03:59<08:19, 10.98it/s]\u001B[A\n",
      "Evaluating:  36%|███▋      | 3145/8622 [04:00<07:34, 12.05it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3153/8622 [04:00<07:11, 12.67it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3161/8622 [04:01<06:44, 13.50it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3169/8622 [04:01<05:55, 15.35it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3177/8622 [04:02<05:26, 16.68it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3185/8622 [04:02<05:18, 17.04it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3193/8622 [04:03<07:01, 12.88it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3201/8622 [04:04<07:51, 11.49it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3209/8622 [04:05<08:43, 10.34it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3217/8622 [04:06<08:44, 10.30it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3225/8622 [04:06<08:51, 10.16it/s]\u001B[A\n",
      "Evaluating:  37%|███▋      | 3233/8622 [04:07<08:06, 11.07it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3241/8622 [04:08<07:55, 11.31it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3249/8622 [04:08<07:07, 12.58it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3257/8622 [04:09<06:27, 13.86it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3265/8622 [04:09<06:05, 14.66it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3273/8622 [04:10<05:50, 15.25it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3281/8622 [04:10<05:49, 15.29it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3289/8622 [04:11<05:38, 15.74it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3297/8622 [04:11<05:33, 15.95it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3305/8622 [04:11<05:15, 16.87it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3313/8622 [04:12<05:07, 17.26it/s]\u001B[A\n",
      "Evaluating:  39%|███▊      | 3321/8622 [04:12<04:35, 19.22it/s]\u001B[A\n",
      "Evaluating:  39%|███▊      | 3329/8622 [04:12<04:09, 21.25it/s]\u001B[A\n",
      "Evaluating:  39%|███▊      | 3337/8622 [04:13<04:11, 21.05it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3345/8622 [04:13<04:36, 19.10it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3353/8622 [04:14<04:29, 19.55it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3361/8622 [04:14<04:29, 19.52it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3369/8622 [04:15<04:23, 19.93it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3377/8622 [04:15<05:33, 15.72it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3385/8622 [04:16<06:26, 13.54it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3393/8622 [04:17<06:05, 14.32it/s]\u001B[A\n",
      "Evaluating:  39%|███▉      | 3401/8622 [04:17<05:59, 14.51it/s]\u001B[A\n",
      "Evaluating:  40%|███▉      | 3409/8622 [04:18<06:09, 14.13it/s]\u001B[A\n",
      "Evaluating:  40%|███▉      | 3417/8622 [04:18<06:16, 13.82it/s]\u001B[A\n",
      "Evaluating:  40%|███▉      | 3425/8622 [04:19<07:00, 12.36it/s]\u001B[A\n",
      "Evaluating:  40%|███▉      | 3433/8622 [04:20<07:59, 10.82it/s]\u001B[A\n",
      "Evaluating:  40%|███▉      | 3441/8622 [04:21<07:40, 11.25it/s]\u001B[A\n",
      "Evaluating:  40%|████      | 3449/8622 [04:21<07:42, 11.19it/s]\u001B[A\n",
      "Evaluating:  40%|████      | 3457/8622 [04:22<07:57, 10.81it/s]\u001B[A\n",
      "Evaluating:  40%|████      | 3465/8622 [04:23<08:05, 10.63it/s]\u001B[A\n",
      "Evaluating:  40%|████      | 3473/8622 [04:24<08:39,  9.92it/s]\u001B[A\n",
      "Evaluating:  40%|████      | 3481/8622 [04:25<07:54, 10.84it/s]\u001B[A\n",
      "Evaluating:  40%|████      | 3489/8622 [04:25<08:23, 10.19it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3497/8622 [04:27<10:27,  8.16it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3505/8622 [04:28<11:03,  7.71it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3513/8622 [04:29<10:30,  8.11it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3521/8622 [04:29<08:52,  9.58it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3529/8622 [04:30<07:44, 10.96it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3537/8622 [04:30<07:03, 12.01it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3545/8622 [04:31<06:14, 13.54it/s]\u001B[A\n",
      "Evaluating:  41%|████      | 3553/8622 [04:31<06:25, 13.16it/s]\u001B[A\n",
      "Evaluating:  41%|████▏     | 3561/8622 [04:32<06:52, 12.25it/s]\u001B[A\n",
      "Evaluating:  41%|████▏     | 3569/8622 [04:33<06:50, 12.32it/s]\u001B[A\n",
      "Evaluating:  41%|████▏     | 3577/8622 [04:33<06:33, 12.81it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3585/8622 [04:34<05:51, 14.32it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3593/8622 [04:35<06:33, 12.79it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3601/8622 [04:35<06:46, 12.35it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3609/8622 [04:36<05:43, 14.58it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3617/8622 [04:36<04:53, 17.07it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3625/8622 [04:36<04:35, 18.12it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3633/8622 [04:37<05:10, 16.08it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3641/8622 [04:38<06:07, 13.57it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3649/8622 [04:38<06:23, 12.98it/s]\u001B[A\n",
      "Evaluating:  42%|████▏     | 3657/8622 [04:39<05:49, 14.19it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3665/8622 [04:39<05:44, 14.39it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3673/8622 [04:40<05:32, 14.90it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3681/8622 [04:40<05:07, 16.05it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3689/8622 [04:41<04:46, 17.24it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3697/8622 [04:41<04:31, 18.12it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3705/8622 [04:42<05:46, 14.18it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3713/8622 [04:43<06:21, 12.86it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3721/8622 [04:43<06:42, 12.19it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3729/8622 [04:44<05:40, 14.39it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3737/8622 [04:44<05:08, 15.86it/s]\u001B[A\n",
      "Evaluating:  43%|████▎     | 3745/8622 [04:44<04:42, 17.28it/s]\u001B[A\n",
      "Evaluating:  44%|████▎     | 3753/8622 [04:45<04:29, 18.07it/s]\u001B[A\n",
      "Evaluating:  44%|████▎     | 3761/8622 [04:45<04:21, 18.58it/s]\u001B[A\n",
      "Evaluating:  44%|████▎     | 3769/8622 [04:46<04:21, 18.56it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3777/8622 [04:46<04:20, 18.62it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3785/8622 [04:47<04:24, 18.26it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3793/8622 [04:47<04:13, 19.01it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3801/8622 [04:47<04:11, 19.19it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3809/8622 [04:48<03:55, 20.41it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3817/8622 [04:48<03:55, 20.36it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3825/8622 [04:49<04:26, 17.98it/s]\u001B[A\n",
      "Evaluating:  44%|████▍     | 3833/8622 [04:49<04:08, 19.29it/s]\u001B[A\n",
      "Evaluating:  45%|████▍     | 3841/8622 [04:49<03:52, 20.55it/s]\u001B[A\n",
      "Evaluating:  45%|████▍     | 3849/8622 [04:50<03:41, 21.52it/s]\u001B[A\n",
      "Evaluating:  45%|████▍     | 3857/8622 [04:50<03:48, 20.88it/s]\u001B[A\n",
      "Evaluating:  45%|████▍     | 3865/8622 [04:51<04:40, 16.95it/s]\u001B[A\n",
      "Evaluating:  45%|████▍     | 3873/8622 [04:51<05:23, 14.68it/s]\u001B[A\n",
      "Evaluating:  45%|████▌     | 3881/8622 [04:52<06:50, 11.54it/s]\u001B[A\n",
      "Evaluating:  45%|████▌     | 3889/8622 [04:53<07:36, 10.37it/s]\u001B[A\n",
      "Evaluating:  45%|████▌     | 3897/8622 [04:54<07:41, 10.23it/s]\u001B[A\n",
      "Evaluating:  45%|████▌     | 3905/8622 [04:55<07:08, 11.01it/s]\u001B[A\n",
      "Evaluating:  45%|████▌     | 3913/8622 [04:55<06:49, 11.50it/s]\u001B[A\n",
      "Evaluating:  45%|████▌     | 3921/8622 [04:56<06:36, 11.84it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3929/8622 [04:57<06:18, 12.39it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3937/8622 [04:57<05:48, 13.45it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3945/8622 [04:58<05:48, 13.42it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3953/8622 [04:58<05:48, 13.38it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3961/8622 [04:59<05:41, 13.66it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3969/8622 [05:00<06:41, 11.60it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3977/8622 [05:02<10:28,  7.39it/s]\u001B[A\n",
      "Evaluating:  46%|████▌     | 3985/8622 [05:04<14:04,  5.49it/s]\u001B[A\n",
      "Evaluating:  46%|████▋     | 3993/8622 [05:07<17:18,  4.46it/s]\u001B[A\n",
      "Evaluating:  46%|████▋     | 4001/8622 [05:09<17:23,  4.43it/s]\u001B[A\n",
      "Evaluating:  46%|████▋     | 4009/8622 [05:10<16:20,  4.70it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4017/8622 [05:11<14:55,  5.14it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4025/8622 [05:12<11:36,  6.60it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4033/8622 [05:12<09:59,  7.65it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4041/8622 [05:13<07:58,  9.57it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4049/8622 [05:13<06:53, 11.06it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4057/8622 [05:14<05:54, 12.88it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4065/8622 [05:14<06:26, 11.78it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4073/8622 [05:15<06:26, 11.77it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4081/8622 [05:16<05:55, 12.76it/s]\u001B[A\n",
      "Evaluating:  47%|████▋     | 4089/8622 [05:16<05:28, 13.80it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4097/8622 [05:16<05:01, 15.01it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4105/8622 [05:17<04:40, 16.12it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4113/8622 [05:17<04:27, 16.87it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4121/8622 [05:18<04:48, 15.59it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4129/8622 [05:18<04:42, 15.92it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4137/8622 [05:19<04:32, 16.48it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4145/8622 [05:19<04:28, 16.66it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4153/8622 [05:20<04:46, 15.61it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4161/8622 [05:21<05:15, 14.13it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4169/8622 [05:21<05:47, 12.81it/s]\u001B[A\n",
      "Evaluating:  48%|████▊     | 4177/8622 [05:22<05:09, 14.36it/s]\u001B[A\n",
      "Evaluating:  49%|████▊     | 4185/8622 [05:22<04:55, 15.01it/s]\u001B[A\n",
      "Evaluating:  49%|████▊     | 4193/8622 [05:23<04:41, 15.74it/s]\u001B[A\n",
      "Evaluating:  49%|████▊     | 4201/8622 [05:23<04:21, 16.94it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4209/8622 [05:23<04:15, 17.28it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4217/8622 [05:24<04:24, 16.65it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4225/8622 [05:25<04:43, 15.50it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4233/8622 [05:25<04:25, 16.55it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4241/8622 [05:25<03:58, 18.34it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4249/8622 [05:26<04:20, 16.81it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4257/8622 [05:26<04:20, 16.74it/s]\u001B[A\n",
      "Evaluating:  49%|████▉     | 4265/8622 [05:27<04:20, 16.72it/s]\u001B[A\n",
      "Evaluating:  50%|████▉     | 4273/8622 [05:27<04:19, 16.76it/s]\u001B[A\n",
      "Evaluating:  50%|████▉     | 4281/8622 [05:28<03:57, 18.28it/s]\u001B[A\n",
      "Evaluating:  50%|████▉     | 4289/8622 [05:28<04:54, 14.73it/s]\u001B[A\n",
      "Evaluating:  50%|████▉     | 4297/8622 [05:29<04:43, 15.28it/s]\u001B[A\n",
      "Evaluating:  50%|████▉     | 4305/8622 [05:29<04:28, 16.08it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4313/8622 [05:30<05:33, 12.93it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4321/8622 [05:31<06:02, 11.85it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4329/8622 [05:32<06:01, 11.89it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4337/8622 [05:33<06:50, 10.43it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4345/8622 [05:34<07:24,  9.63it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4353/8622 [05:34<06:30, 10.94it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4361/8622 [05:35<05:51, 12.12it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4369/8622 [05:36<06:19, 11.21it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4377/8622 [05:36<06:09, 11.48it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4385/8622 [05:37<06:02, 11.70it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4393/8622 [05:37<05:55, 11.91it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4401/8622 [05:38<06:07, 11.48it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4409/8622 [05:39<06:08, 11.43it/s]\u001B[A\n",
      "Evaluating:  51%|█████     | 4417/8622 [05:40<07:33,  9.28it/s]\u001B[A\n",
      "Evaluating:  51%|█████▏    | 4425/8622 [05:41<07:01,  9.95it/s]\u001B[A\n",
      "Evaluating:  51%|█████▏    | 4433/8622 [05:41<06:33, 10.65it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4441/8622 [05:42<06:20, 10.98it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4449/8622 [05:43<05:51, 11.87it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4457/8622 [05:44<07:05,  9.78it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4465/8622 [05:46<09:38,  7.19it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4473/8622 [05:46<08:17,  8.34it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4481/8622 [05:47<07:38,  9.03it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4489/8622 [05:48<07:05,  9.70it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4497/8622 [05:48<06:23, 10.75it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4505/8622 [05:49<05:50, 11.74it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4513/8622 [05:49<05:51, 11.70it/s]\u001B[A\n",
      "Evaluating:  52%|█████▏    | 4521/8622 [05:50<05:46, 11.83it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4529/8622 [05:51<05:33, 12.26it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4537/8622 [05:51<05:38, 12.05it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4545/8622 [05:52<06:15, 10.85it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4553/8622 [05:53<06:01, 11.24it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4561/8622 [05:54<06:23, 10.59it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4569/8622 [05:55<06:30, 10.38it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4577/8622 [05:55<06:27, 10.44it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4585/8622 [05:56<06:30, 10.34it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4593/8622 [05:57<06:54,  9.72it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4601/8622 [05:58<07:02,  9.52it/s]\u001B[A\n",
      "Evaluating:  53%|█████▎    | 4609/8622 [05:59<06:51,  9.75it/s]\u001B[A\n",
      "Evaluating:  54%|█████▎    | 4617/8622 [05:59<05:30, 12.12it/s]\u001B[A\n",
      "Evaluating:  54%|█████▎    | 4625/8622 [05:59<04:46, 13.96it/s]\u001B[A\n",
      "Evaluating:  54%|█████▎    | 4633/8622 [06:00<04:14, 15.67it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4641/8622 [06:00<03:44, 17.73it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4649/8622 [06:00<03:21, 19.75it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4657/8622 [06:01<03:51, 17.13it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4665/8622 [06:01<03:47, 17.43it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4673/8622 [06:03<05:52, 11.21it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4681/8622 [06:04<07:08,  9.20it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4689/8622 [06:05<06:51,  9.56it/s]\u001B[A\n",
      "Evaluating:  54%|█████▍    | 4697/8622 [06:06<07:21,  8.89it/s]\u001B[A\n",
      "Evaluating:  55%|█████▍    | 4705/8622 [06:07<07:38,  8.54it/s]\u001B[A\n",
      "Evaluating:  55%|█████▍    | 4713/8622 [06:08<07:45,  8.40it/s]\u001B[A\n",
      "Evaluating:  55%|█████▍    | 4721/8622 [06:09<07:28,  8.70it/s]\u001B[A\n",
      "Evaluating:  55%|█████▍    | 4729/8622 [06:10<07:26,  8.72it/s]\u001B[A\n",
      "Evaluating:  55%|█████▍    | 4737/8622 [06:11<08:29,  7.63it/s]\u001B[A\n",
      "Evaluating:  55%|█████▌    | 4745/8622 [06:12<07:28,  8.65it/s]\u001B[A\n",
      "Evaluating:  55%|█████▌    | 4753/8622 [06:13<07:49,  8.25it/s]\u001B[A\n",
      "Evaluating:  55%|█████▌    | 4761/8622 [06:14<07:46,  8.27it/s]\u001B[A\n",
      "Evaluating:  55%|█████▌    | 4769/8622 [06:14<06:53,  9.32it/s]\u001B[A\n",
      "Evaluating:  55%|█████▌    | 4777/8622 [06:15<07:18,  8.78it/s]\u001B[A\n",
      "Evaluating:  55%|█████▌    | 4785/8622 [06:16<07:38,  8.37it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4793/8622 [06:17<08:02,  7.93it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4801/8622 [06:19<09:21,  6.80it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4809/8622 [06:20<09:14,  6.87it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4817/8622 [06:22<10:05,  6.28it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4825/8622 [06:24<11:37,  5.44it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4833/8622 [06:25<10:40,  5.92it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4841/8622 [06:26<09:51,  6.40it/s]\u001B[A\n",
      "Evaluating:  56%|█████▌    | 4849/8622 [06:27<10:58,  5.73it/s]\u001B[A\n",
      "Evaluating:  56%|█████▋    | 4857/8622 [06:30<13:14,  4.74it/s]\u001B[A\n",
      "Evaluating:  56%|█████▋    | 4865/8622 [06:31<11:05,  5.64it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4873/8622 [06:31<09:18,  6.71it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4881/8622 [06:32<07:38,  8.16it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4889/8622 [06:33<07:48,  7.96it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4897/8622 [06:34<08:50,  7.03it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4905/8622 [06:35<08:21,  7.41it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4913/8622 [06:36<07:26,  8.30it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4921/8622 [06:36<06:23,  9.64it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4929/8622 [06:37<06:26,  9.56it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4937/8622 [06:39<07:53,  7.79it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4945/8622 [06:39<06:36,  9.27it/s]\u001B[A\n",
      "Evaluating:  57%|█████▋    | 4953/8622 [06:40<05:46, 10.58it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 4961/8622 [06:40<05:03, 12.07it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 4969/8622 [06:40<04:23, 13.87it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 4977/8622 [06:41<04:18, 14.09it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 4985/8622 [06:42<04:25, 13.72it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 4993/8622 [06:42<03:45, 16.13it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 5001/8622 [06:42<03:31, 17.12it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 5009/8622 [06:43<03:17, 18.31it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 5017/8622 [06:43<03:13, 18.66it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 5025/8622 [06:44<03:08, 19.05it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 5033/8622 [06:44<03:11, 18.76it/s]\u001B[A\n",
      "Evaluating:  58%|█████▊    | 5041/8622 [06:44<03:09, 18.88it/s]\u001B[A\n",
      "Evaluating:  59%|█████▊    | 5049/8622 [06:45<02:54, 20.48it/s]\u001B[A\n",
      "Evaluating:  59%|█████▊    | 5057/8622 [06:45<03:05, 19.18it/s]\u001B[A\n",
      "Evaluating:  59%|█████▊    | 5065/8622 [06:46<03:09, 18.73it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5073/8622 [06:46<03:16, 18.02it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5081/8622 [06:47<03:15, 18.09it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5089/8622 [06:47<03:17, 17.91it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5097/8622 [06:47<03:21, 17.45it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5105/8622 [06:48<03:32, 16.58it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5113/8622 [06:48<03:19, 17.56it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5121/8622 [06:49<03:35, 16.21it/s]\u001B[A\n",
      "Evaluating:  59%|█████▉    | 5129/8622 [06:50<04:08, 14.06it/s]\u001B[A\n",
      "Evaluating:  60%|█████▉    | 5137/8622 [06:51<06:20,  9.16it/s]\u001B[A\n",
      "Evaluating:  60%|█████▉    | 5145/8622 [06:52<06:06,  9.50it/s]\u001B[A\n",
      "Evaluating:  60%|█████▉    | 5153/8622 [06:53<05:47,  9.97it/s]\u001B[A\n",
      "Evaluating:  60%|█████▉    | 5161/8622 [06:54<06:52,  8.39it/s]\u001B[A\n",
      "Evaluating:  60%|█████▉    | 5169/8622 [06:55<07:12,  7.99it/s]\u001B[A\n",
      "Evaluating:  60%|██████    | 5177/8622 [06:56<06:50,  8.40it/s]\u001B[A\n",
      "Evaluating:  60%|██████    | 5185/8622 [06:57<07:51,  7.29it/s]\u001B[A\n",
      "Evaluating:  60%|██████    | 5193/8622 [06:58<07:15,  7.87it/s]\u001B[A\n",
      "Evaluating:  60%|██████    | 5201/8622 [06:59<07:06,  8.03it/s]\u001B[A\n",
      "Evaluating:  60%|██████    | 5209/8622 [07:00<06:29,  8.76it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5217/8622 [07:01<06:14,  9.10it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5225/8622 [07:01<05:48,  9.75it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5233/8622 [07:02<05:57,  9.49it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5241/8622 [07:03<06:11,  9.11it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5249/8622 [07:04<06:44,  8.34it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5257/8622 [07:06<08:15,  6.79it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5265/8622 [07:08<08:42,  6.43it/s]\u001B[A\n",
      "Evaluating:  61%|██████    | 5273/8622 [07:09<08:39,  6.45it/s]\u001B[A\n",
      "Evaluating:  61%|██████▏   | 5281/8622 [07:10<07:47,  7.14it/s]\u001B[A\n",
      "Evaluating:  61%|██████▏   | 5289/8622 [07:11<07:43,  7.19it/s]\u001B[A\n",
      "Evaluating:  61%|██████▏   | 5297/8622 [07:11<06:41,  8.29it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5305/8622 [07:12<06:20,  8.72it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5313/8622 [07:14<07:23,  7.45it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5321/8622 [07:15<07:13,  7.62it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5329/8622 [07:16<07:12,  7.62it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5337/8622 [07:17<08:17,  6.60it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5345/8622 [07:18<07:28,  7.31it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5353/8622 [07:19<07:15,  7.50it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5361/8622 [07:20<07:27,  7.29it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5369/8622 [07:21<07:35,  7.15it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5377/8622 [07:22<07:27,  7.25it/s]\u001B[A\n",
      "Evaluating:  62%|██████▏   | 5385/8622 [07:23<07:09,  7.53it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5393/8622 [07:25<07:12,  7.47it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5401/8622 [07:26<08:49,  6.09it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5409/8622 [07:27<08:02,  6.66it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5417/8622 [07:29<08:04,  6.61it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5425/8622 [07:30<07:48,  6.83it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5433/8622 [07:31<08:26,  6.30it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5441/8622 [07:32<07:29,  7.08it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5449/8622 [07:33<06:49,  7.76it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5457/8622 [07:34<06:28,  8.15it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5465/8622 [07:35<06:42,  7.84it/s]\u001B[A\n",
      "Evaluating:  63%|██████▎   | 5473/8622 [07:36<06:51,  7.65it/s]\u001B[A\n",
      "Evaluating:  64%|██████▎   | 5481/8622 [07:37<06:36,  7.93it/s]\u001B[A\n",
      "Evaluating:  64%|██████▎   | 5489/8622 [07:38<06:05,  8.57it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5497/8622 [07:38<05:48,  8.97it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5505/8622 [07:39<05:49,  8.93it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5513/8622 [07:40<06:25,  8.07it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5521/8622 [07:42<06:36,  7.82it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5529/8622 [07:42<06:18,  8.18it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5537/8622 [07:43<05:28,  9.38it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5545/8622 [07:44<05:19,  9.62it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5553/8622 [07:45<05:39,  9.04it/s]\u001B[A\n",
      "Evaluating:  64%|██████▍   | 5561/8622 [07:46<06:21,  8.03it/s]\u001B[A\n",
      "Evaluating:  65%|██████▍   | 5569/8622 [07:47<07:13,  7.04it/s]\u001B[A\n",
      "Evaluating:  65%|██████▍   | 5577/8622 [07:48<06:29,  7.81it/s]\u001B[A\n",
      "Evaluating:  65%|██████▍   | 5585/8622 [07:49<05:39,  8.94it/s]\u001B[A\n",
      "Evaluating:  65%|██████▍   | 5593/8622 [07:49<05:05,  9.93it/s]\u001B[A\n",
      "Evaluating:  65%|██████▍   | 5601/8622 [07:50<05:05,  9.90it/s]\u001B[A\n",
      "Evaluating:  65%|██████▌   | 5609/8622 [07:51<04:48, 10.44it/s]\u001B[A\n",
      "Evaluating:  65%|██████▌   | 5617/8622 [07:52<04:51, 10.33it/s]\u001B[A\n",
      "Evaluating:  65%|██████▌   | 5625/8622 [07:52<04:40, 10.67it/s]\u001B[A\n",
      "Evaluating:  65%|██████▌   | 5633/8622 [07:53<04:45, 10.45it/s]\u001B[A\n",
      "Evaluating:  65%|██████▌   | 5641/8622 [07:54<04:46, 10.41it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5649/8622 [07:55<04:35, 10.80it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5657/8622 [07:55<04:30, 10.96it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5665/8622 [07:56<04:25, 11.13it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5673/8622 [07:57<04:52, 10.09it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5681/8622 [07:58<04:52, 10.07it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5689/8622 [07:58<04:37, 10.56it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5697/8622 [07:59<04:58,  9.80it/s]\u001B[A\n",
      "Evaluating:  66%|██████▌   | 5705/8622 [08:00<05:13,  9.32it/s]\u001B[A\n",
      "Evaluating:  66%|██████▋   | 5713/8622 [08:02<05:40,  8.53it/s]\u001B[A\n",
      "Evaluating:  66%|██████▋   | 5721/8622 [08:02<05:37,  8.60it/s]\u001B[A\n",
      "Evaluating:  66%|██████▋   | 5729/8622 [08:03<05:44,  8.40it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5737/8622 [08:05<05:59,  8.02it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5745/8622 [08:06<06:31,  7.35it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5753/8622 [08:07<07:10,  6.67it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5761/8622 [08:08<06:38,  7.19it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5769/8622 [08:09<06:09,  7.71it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5777/8622 [08:10<06:10,  7.68it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5785/8622 [08:11<05:49,  8.12it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5793/8622 [08:12<05:48,  8.12it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5801/8622 [08:14<06:57,  6.75it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5809/8622 [08:15<06:32,  7.17it/s]\u001B[A\n",
      "Evaluating:  67%|██████▋   | 5817/8622 [08:16<06:31,  7.17it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5825/8622 [08:17<06:42,  6.95it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5833/8622 [08:18<06:16,  7.40it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5841/8622 [08:19<06:39,  6.97it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5849/8622 [08:20<06:26,  7.17it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5857/8622 [08:21<06:10,  7.46it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5865/8622 [08:22<05:55,  7.75it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5873/8622 [08:23<06:07,  7.47it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5881/8622 [08:24<06:12,  7.35it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5889/8622 [08:26<07:21,  6.19it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5897/8622 [08:27<07:23,  6.15it/s]\u001B[A\n",
      "Evaluating:  68%|██████▊   | 5905/8622 [08:29<08:17,  5.47it/s]\u001B[A\n",
      "Evaluating:  69%|██████▊   | 5913/8622 [08:31<07:53,  5.72it/s]\u001B[A\n",
      "Evaluating:  69%|██████▊   | 5921/8622 [08:32<07:22,  6.10it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5929/8622 [08:33<06:36,  6.79it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5937/8622 [08:33<05:36,  7.98it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5945/8622 [08:34<05:17,  8.43it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5953/8622 [08:35<04:49,  9.23it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5961/8622 [08:35<04:43,  9.39it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5969/8622 [08:36<04:37,  9.57it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5977/8622 [08:37<04:42,  9.37it/s]\u001B[A\n",
      "Evaluating:  69%|██████▉   | 5985/8622 [08:38<04:52,  9.00it/s]\u001B[A\n",
      "Evaluating:  70%|██████▉   | 5993/8622 [08:39<05:11,  8.43it/s]\u001B[A\n",
      "Evaluating:  70%|██████▉   | 6001/8622 [08:40<05:06,  8.54it/s]\u001B[A\n",
      "Evaluating:  70%|██████▉   | 6009/8622 [08:41<04:50,  8.99it/s]\u001B[A\n",
      "Evaluating:  70%|██████▉   | 6017/8622 [08:42<04:38,  9.35it/s]\u001B[A\n",
      "Evaluating:  70%|██████▉   | 6025/8622 [08:43<05:07,  8.45it/s]\u001B[A\n",
      "Evaluating:  70%|██████▉   | 6033/8622 [08:44<05:10,  8.34it/s]\u001B[A\n",
      "Evaluating:  70%|███████   | 6041/8622 [08:45<04:47,  8.99it/s]\u001B[A\n",
      "Evaluating:  70%|███████   | 6049/8622 [08:45<04:44,  9.03it/s]\u001B[A\n",
      "Evaluating:  70%|███████   | 6057/8622 [08:46<04:42,  9.08it/s]\u001B[A\n",
      "Evaluating:  70%|███████   | 6065/8622 [08:47<04:40,  9.12it/s]\u001B[A\n",
      "Evaluating:  70%|███████   | 6073/8622 [08:48<04:33,  9.31it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6081/8622 [08:49<04:47,  8.84it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6089/8622 [08:50<05:11,  8.14it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6097/8622 [08:51<05:01,  8.38it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6105/8622 [08:52<04:48,  8.71it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6113/8622 [08:53<04:24,  9.49it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6121/8622 [08:54<04:53,  8.53it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6129/8622 [08:55<05:30,  7.54it/s]\u001B[A\n",
      "Evaluating:  71%|███████   | 6137/8622 [08:56<05:37,  7.35it/s]\u001B[A\n",
      "Evaluating:  71%|███████▏  | 6145/8622 [08:57<05:58,  6.92it/s]\u001B[A\n",
      "Evaluating:  71%|███████▏  | 6153/8622 [08:58<05:32,  7.44it/s]\u001B[A\n",
      "Evaluating:  71%|███████▏  | 6161/8622 [09:00<05:52,  6.97it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6169/8622 [09:01<05:41,  7.17it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6177/8622 [09:02<05:41,  7.17it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6185/8622 [09:03<05:36,  7.25it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6193/8622 [09:04<05:21,  7.57it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6201/8622 [09:05<05:19,  7.57it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6209/8622 [09:06<05:04,  7.93it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6217/8622 [09:07<05:03,  7.92it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6225/8622 [09:09<06:07,  6.52it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6233/8622 [09:10<06:47,  5.87it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6241/8622 [09:12<06:46,  5.86it/s]\u001B[A\n",
      "Evaluating:  72%|███████▏  | 6249/8622 [09:14<08:13,  4.81it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6257/8622 [09:15<07:45,  5.08it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6265/8622 [09:17<07:17,  5.38it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6273/8622 [09:18<06:37,  5.91it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6281/8622 [09:19<06:14,  6.25it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6289/8622 [09:20<05:58,  6.51it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6297/8622 [09:21<05:26,  7.13it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6305/8622 [09:22<05:11,  7.44it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6313/8622 [09:23<05:05,  7.56it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6321/8622 [09:24<04:40,  8.21it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6329/8622 [09:25<04:51,  7.87it/s]\u001B[A\n",
      "Evaluating:  73%|███████▎  | 6337/8622 [09:26<05:25,  7.01it/s]\u001B[A\n",
      "Evaluating:  74%|███████▎  | 6345/8622 [09:28<05:52,  6.46it/s]\u001B[A\n",
      "Evaluating:  74%|███████▎  | 6353/8622 [09:29<06:10,  6.12it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6361/8622 [09:30<05:54,  6.38it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6369/8622 [09:31<05:39,  6.65it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6377/8622 [09:32<05:05,  7.34it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6385/8622 [09:33<05:06,  7.30it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6393/8622 [09:34<04:50,  7.66it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6401/8622 [09:35<04:24,  8.38it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6409/8622 [09:36<04:01,  9.17it/s]\u001B[A\n",
      "Evaluating:  74%|███████▍  | 6417/8622 [09:36<04:01,  9.11it/s]\u001B[A\n",
      "Evaluating:  75%|███████▍  | 6425/8622 [09:37<04:08,  8.83it/s]\u001B[A\n",
      "Evaluating:  75%|███████▍  | 6433/8622 [09:38<04:11,  8.71it/s]\u001B[A\n",
      "Evaluating:  75%|███████▍  | 6441/8622 [09:40<04:37,  7.86it/s]\u001B[A\n",
      "Evaluating:  75%|███████▍  | 6449/8622 [09:42<06:39,  5.44it/s]\u001B[A\n",
      "Evaluating:  75%|███████▍  | 6457/8622 [09:47<11:33,  3.12it/s]\u001B[A\n",
      "Evaluating:  75%|███████▍  | 6465/8622 [09:52<14:34,  2.47it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6473/8622 [09:57<16:22,  2.19it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6481/8622 [10:01<17:20,  2.06it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6489/8622 [10:04<16:04,  2.21it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6497/8622 [10:07<15:25,  2.30it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6505/8622 [10:11<15:17,  2.31it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6513/8622 [10:14<14:51,  2.37it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6521/8622 [10:17<14:15,  2.46it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6529/8622 [10:20<13:51,  2.52it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6537/8622 [10:22<12:33,  2.77it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6545/8622 [10:26<14:10,  2.44it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6553/8622 [10:32<17:08,  2.01it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6561/8622 [10:35<16:25,  2.09it/s]\u001B[A\n",
      "Evaluating:  76%|███████▌  | 6569/8622 [10:38<15:29,  2.21it/s]\u001B[A\n",
      "Evaluating:  76%|███████▋  | 6577/8622 [10:43<16:07,  2.11it/s]\u001B[A\n",
      "Evaluating:  76%|███████▋  | 6585/8622 [10:45<14:55,  2.28it/s]\u001B[A\n",
      "Evaluating:  76%|███████▋  | 6593/8622 [10:48<13:40,  2.47it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6601/8622 [10:50<12:11,  2.76it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6609/8622 [10:53<11:53,  2.82it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6617/8622 [10:56<11:50,  2.82it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6625/8622 [10:58<11:12,  2.97it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6633/8622 [11:00<10:42,  3.10it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6641/8622 [11:03<10:30,  3.14it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6649/8622 [11:06<10:40,  3.08it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6657/8622 [11:08<10:45,  3.04it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6665/8622 [11:12<12:33,  2.60it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6673/8622 [11:16<12:50,  2.53it/s]\u001B[A\n",
      "Evaluating:  77%|███████▋  | 6681/8622 [11:19<12:25,  2.60it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6689/8622 [11:22<12:18,  2.62it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6697/8622 [11:26<13:23,  2.40it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6705/8622 [11:28<12:27,  2.56it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6713/8622 [11:31<11:59,  2.65it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6721/8622 [11:35<12:39,  2.50it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6729/8622 [11:38<12:35,  2.50it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6737/8622 [11:41<12:21,  2.54it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6745/8622 [11:44<12:16,  2.55it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6753/8622 [11:48<13:10,  2.36it/s]\u001B[A\n",
      "Evaluating:  78%|███████▊  | 6761/8622 [11:52<14:06,  2.20it/s]\u001B[A\n",
      "Evaluating:  79%|███████▊  | 6769/8622 [11:55<13:17,  2.32it/s]\u001B[A\n",
      "Evaluating:  79%|███████▊  | 6777/8622 [11:58<12:23,  2.48it/s]\u001B[A\n",
      "Evaluating:  79%|███████▊  | 6785/8622 [12:01<11:50,  2.59it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6793/8622 [12:04<11:52,  2.57it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6801/8622 [12:05<09:58,  3.04it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6809/8622 [12:06<08:08,  3.71it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6817/8622 [12:07<06:33,  4.59it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6825/8622 [12:08<05:55,  5.05it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6833/8622 [12:09<05:01,  5.92it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6841/8622 [12:10<04:43,  6.28it/s]\u001B[A\n",
      "Evaluating:  79%|███████▉  | 6849/8622 [12:11<03:59,  7.40it/s]\u001B[A\n",
      "Evaluating:  80%|███████▉  | 6857/8622 [12:12<04:16,  6.88it/s]\u001B[A\n",
      "Evaluating:  80%|███████▉  | 6865/8622 [12:13<04:03,  7.21it/s]\u001B[A\n",
      "Evaluating:  80%|███████▉  | 6873/8622 [12:15<04:24,  6.62it/s]\u001B[A\n",
      "Evaluating:  80%|███████▉  | 6881/8622 [12:16<04:03,  7.14it/s]\u001B[A\n",
      "Evaluating:  80%|███████▉  | 6889/8622 [12:17<04:18,  6.70it/s]\u001B[A\n",
      "Evaluating:  80%|███████▉  | 6897/8622 [12:19<04:55,  5.84it/s]\u001B[A\n",
      "Evaluating:  80%|████████  | 6905/8622 [12:20<05:10,  5.52it/s]\u001B[A\n",
      "Evaluating:  80%|████████  | 6913/8622 [12:22<05:42,  4.99it/s]\u001B[A\n",
      "Evaluating:  80%|████████  | 6921/8622 [12:24<05:42,  4.96it/s]\u001B[A\n",
      "Evaluating:  80%|████████  | 6929/8622 [12:26<05:46,  4.89it/s]\u001B[A\n",
      "Evaluating:  80%|████████  | 6937/8622 [12:28<05:58,  4.70it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 6945/8622 [12:29<05:30,  5.08it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 6953/8622 [12:30<04:48,  5.78it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 6961/8622 [12:31<04:15,  6.49it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 6969/8622 [12:32<04:00,  6.87it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 6977/8622 [12:33<04:16,  6.42it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 6985/8622 [12:35<05:00,  5.44it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 6993/8622 [12:36<04:51,  5.58it/s]\u001B[A\n",
      "Evaluating:  81%|████████  | 7001/8622 [12:37<04:16,  6.31it/s]\u001B[A\n",
      "Evaluating:  81%|████████▏ | 7009/8622 [12:38<03:41,  7.28it/s]\u001B[A\n",
      "Evaluating:  81%|████████▏ | 7017/8622 [12:39<03:57,  6.75it/s]\u001B[A\n",
      "Evaluating:  81%|████████▏ | 7025/8622 [12:41<04:01,  6.60it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7033/8622 [12:43<04:55,  5.37it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7041/8622 [12:46<07:02,  3.74it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7049/8622 [12:48<06:27,  4.06it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7057/8622 [12:49<05:48,  4.48it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7065/8622 [12:51<05:35,  4.64it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7073/8622 [12:52<05:04,  5.09it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7081/8622 [12:53<04:36,  5.57it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7089/8622 [12:55<05:11,  4.92it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7097/8622 [12:58<05:54,  4.31it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7105/8622 [12:59<05:30,  4.60it/s]\u001B[A\n",
      "Evaluating:  82%|████████▏ | 7113/8622 [13:00<04:31,  5.55it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7121/8622 [13:01<03:41,  6.76it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7129/8622 [13:01<03:21,  7.41it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7137/8622 [13:03<03:26,  7.19it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7145/8622 [13:04<03:36,  6.81it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7153/8622 [13:05<03:17,  7.45it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7161/8622 [13:05<02:54,  8.35it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7169/8622 [13:06<02:46,  8.75it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7177/8622 [13:07<02:28,  9.73it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7185/8622 [13:08<02:28,  9.66it/s]\u001B[A\n",
      "Evaluating:  83%|████████▎ | 7193/8622 [13:08<02:18, 10.34it/s]\u001B[A\n",
      "Evaluating:  84%|████████▎ | 7201/8622 [13:09<01:50, 12.84it/s]\u001B[A\n",
      "Evaluating:  84%|████████▎ | 7209/8622 [13:09<01:37, 14.43it/s]\u001B[A\n",
      "Evaluating:  84%|████████▎ | 7217/8622 [13:09<01:31, 15.42it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7225/8622 [13:10<01:19, 17.67it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7233/8622 [13:10<01:24, 16.43it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7241/8622 [13:11<01:17, 17.85it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7249/8622 [13:11<01:08, 20.14it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7257/8622 [13:11<01:06, 20.47it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7265/8622 [13:12<01:04, 21.08it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7273/8622 [13:12<01:12, 18.58it/s]\u001B[A\n",
      "Evaluating:  84%|████████▍ | 7281/8622 [13:13<01:13, 18.28it/s]\u001B[A\n",
      "Evaluating:  85%|████████▍ | 7289/8622 [13:13<01:25, 15.68it/s]\u001B[A\n",
      "Evaluating:  85%|████████▍ | 7297/8622 [13:14<01:38, 13.47it/s]\u001B[A\n",
      "Evaluating:  85%|████████▍ | 7305/8622 [13:15<01:35, 13.72it/s]\u001B[A\n",
      "Evaluating:  85%|████████▍ | 7313/8622 [13:15<01:33, 14.07it/s]\u001B[A\n",
      "Evaluating:  85%|████████▍ | 7321/8622 [13:16<01:24, 15.36it/s]\u001B[A\n",
      "Evaluating:  85%|████████▌ | 7329/8622 [13:16<01:15, 17.05it/s]\u001B[A\n",
      "Evaluating:  85%|████████▌ | 7337/8622 [13:16<01:10, 18.31it/s]\u001B[A\n",
      "Evaluating:  85%|████████▌ | 7345/8622 [13:17<01:03, 20.19it/s]\u001B[A\n",
      "Evaluating:  85%|████████▌ | 7353/8622 [13:17<01:03, 19.84it/s]\u001B[A\n",
      "Evaluating:  85%|████████▌ | 7361/8622 [13:17<01:04, 19.62it/s]\u001B[A\n",
      "Evaluating:  85%|████████▌ | 7369/8622 [13:18<01:08, 18.32it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7377/8622 [13:19<01:16, 16.21it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7385/8622 [13:19<01:27, 14.06it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7393/8622 [13:20<01:18, 15.71it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7401/8622 [13:20<01:11, 17.13it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7409/8622 [13:20<01:06, 18.26it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7417/8622 [13:21<01:04, 18.77it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7425/8622 [13:21<01:02, 19.12it/s]\u001B[A\n",
      "Evaluating:  86%|████████▌ | 7433/8622 [13:22<01:00, 19.70it/s]\u001B[A\n",
      "Evaluating:  86%|████████▋ | 7441/8622 [13:22<01:07, 17.46it/s]\u001B[A\n",
      "Evaluating:  86%|████████▋ | 7449/8622 [13:23<01:20, 14.57it/s]\u001B[A\n",
      "Evaluating:  86%|████████▋ | 7457/8622 [13:24<01:35, 12.16it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7465/8622 [13:25<01:40, 11.50it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7473/8622 [13:25<01:39, 11.58it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7481/8622 [13:26<01:35, 11.99it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7489/8622 [13:27<01:37, 11.64it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7497/8622 [13:27<01:35, 11.81it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7505/8622 [13:29<01:54,  9.73it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7513/8622 [13:29<01:41, 10.95it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7521/8622 [13:30<02:05,  8.78it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7529/8622 [13:32<02:49,  6.44it/s]\u001B[A\n",
      "Evaluating:  87%|████████▋ | 7537/8622 [13:33<02:38,  6.86it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7545/8622 [13:34<02:29,  7.19it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7553/8622 [13:35<02:10,  8.18it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7561/8622 [13:36<01:59,  8.90it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7569/8622 [13:36<01:45, 10.02it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7577/8622 [13:37<01:33, 11.14it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7585/8622 [13:37<01:18, 13.22it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7593/8622 [13:38<01:12, 14.19it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7601/8622 [13:38<01:08, 14.85it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7609/8622 [13:39<01:05, 15.38it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7617/8622 [13:39<01:06, 15.17it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7625/8622 [13:40<01:09, 14.42it/s]\u001B[A\n",
      "Evaluating:  89%|████████▊ | 7633/8622 [13:40<01:08, 14.37it/s]\u001B[A\n",
      "Evaluating:  89%|████████▊ | 7641/8622 [13:42<02:06,  7.73it/s]\u001B[A\n",
      "Evaluating:  89%|████████▊ | 7649/8622 [13:44<02:16,  7.14it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7657/8622 [13:44<01:45,  9.12it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7665/8622 [13:44<01:23, 11.45it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7673/8622 [13:45<01:08, 13.80it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7681/8622 [13:45<00:58, 16.12it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7689/8622 [13:45<00:52, 17.65it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7697/8622 [13:46<00:50, 18.23it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7705/8622 [13:46<00:44, 20.48it/s]\u001B[A\n",
      "Evaluating:  89%|████████▉ | 7713/8622 [13:46<00:43, 20.72it/s]\u001B[A\n",
      "Evaluating:  90%|████████▉ | 7721/8622 [13:47<00:41, 21.48it/s]\u001B[A\n",
      "Evaluating:  90%|████████▉ | 7729/8622 [13:47<00:40, 21.79it/s]\u001B[A\n",
      "Evaluating:  90%|████████▉ | 7737/8622 [13:47<00:40, 21.70it/s]\u001B[A\n",
      "Evaluating:  90%|████████▉ | 7745/8622 [13:48<00:40, 21.69it/s]\u001B[A\n",
      "Evaluating:  90%|████████▉ | 7753/8622 [13:48<00:41, 20.97it/s]\u001B[A\n",
      "Evaluating:  90%|█████████ | 7761/8622 [13:49<00:42, 20.31it/s]\u001B[A\n",
      "Evaluating:  90%|█████████ | 7769/8622 [13:49<00:42, 20.30it/s]\u001B[A\n",
      "Evaluating:  90%|█████████ | 7777/8622 [13:49<00:42, 20.02it/s]\u001B[A\n",
      "Evaluating:  90%|█████████ | 7785/8622 [13:50<00:42, 19.54it/s]\u001B[A\n",
      "Evaluating:  90%|█████████ | 7793/8622 [13:50<00:41, 20.11it/s]\u001B[A\n",
      "Evaluating:  90%|█████████ | 7801/8622 [13:51<00:38, 21.11it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7809/8622 [13:51<00:38, 21.25it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7817/8622 [13:51<00:39, 20.57it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7825/8622 [13:52<00:45, 17.70it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7833/8622 [13:53<01:06, 11.89it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7841/8622 [13:55<01:27,  8.96it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7849/8622 [13:56<01:26,  8.89it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7857/8622 [13:56<01:16,  9.96it/s]\u001B[A\n",
      "Evaluating:  91%|█████████ | 7865/8622 [13:57<01:17,  9.80it/s]\u001B[A\n",
      "Evaluating:  91%|█████████▏| 7873/8622 [13:58<01:12, 10.30it/s]\u001B[A\n",
      "Evaluating:  91%|█████████▏| 7881/8622 [13:59<01:29,  8.27it/s]\u001B[A\n",
      "Evaluating:  91%|█████████▏| 7889/8622 [14:00<01:36,  7.62it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7897/8622 [14:01<01:32,  7.83it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7905/8622 [14:02<01:16,  9.33it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7913/8622 [14:02<01:05, 10.82it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7921/8622 [14:03<01:13,  9.54it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7929/8622 [14:06<02:01,  5.70it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7937/8622 [14:08<02:23,  4.78it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7945/8622 [14:10<02:21,  4.79it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7953/8622 [14:11<02:16,  4.91it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7961/8622 [14:13<02:13,  4.97it/s]\u001B[A\n",
      "Evaluating:  92%|█████████▏| 7969/8622 [14:14<02:04,  5.23it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 7977/8622 [14:15<01:52,  5.72it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 7985/8622 [14:16<01:38,  6.46it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 7993/8622 [14:17<01:23,  7.49it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8001/8622 [14:18<01:26,  7.20it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8009/8622 [14:20<01:32,  6.60it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8017/8622 [14:21<01:42,  5.89it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8025/8622 [14:23<01:46,  5.60it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8033/8622 [14:24<01:28,  6.63it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8041/8622 [14:24<01:19,  7.30it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8049/8622 [14:26<01:26,  6.64it/s]\u001B[A\n",
      "Evaluating:  93%|█████████▎| 8057/8622 [14:27<01:12,  7.83it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▎| 8065/8622 [14:27<00:59,  9.31it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▎| 8073/8622 [14:28<00:52, 10.51it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▎| 8081/8622 [14:28<00:46, 11.65it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▍| 8089/8622 [14:29<00:44, 12.07it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▍| 8097/8622 [14:29<00:39, 13.26it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▍| 8105/8622 [14:30<00:43, 11.89it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▍| 8113/8622 [14:31<00:52,  9.64it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 3861/3861 [19:35<00:00,  3.29it/s]\u001B[A\n",
      "\n",
      "Evaluating:  94%|█████████▍| 8129/8622 [14:33<00:59,  8.32it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▍| 8137/8622 [14:34<00:58,  8.32it/s]\u001B[A\n",
      "Evaluating:  94%|█████████▍| 8145/8622 [14:35<00:59,  8.05it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▍| 8153/8622 [14:36<00:58,  8.02it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▍| 8161/8622 [14:37<00:55,  8.31it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▍| 8169/8622 [14:38<00:56,  8.01it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▍| 8177/8622 [14:40<01:01,  7.29it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▍| 8185/8622 [14:41<00:58,  7.43it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▌| 8193/8622 [14:42<00:55,  7.75it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▌| 8201/8622 [14:43<00:54,  7.68it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▌| 8209/8622 [14:44<01:02,  6.62it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▌| 8217/8622 [14:46<01:03,  6.40it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▌| 8225/8622 [14:47<01:06,  6.00it/s]\u001B[A\n",
      "Evaluating:  95%|█████████▌| 8233/8622 [14:49<01:07,  5.76it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8241/8622 [14:50<01:03,  5.96it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8249/8622 [14:51<01:05,  5.66it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8257/8622 [14:53<01:00,  5.98it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8265/8622 [14:54<00:59,  6.04it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8273/8622 [14:56<01:01,  5.65it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8281/8622 [14:57<01:00,  5.64it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8289/8622 [14:59<01:01,  5.45it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▌| 8297/8622 [15:00<00:58,  5.58it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▋| 8305/8622 [15:01<00:57,  5.54it/s]\u001B[A\n",
      "Evaluating:  96%|█████████▋| 8313/8622 [15:03<00:57,  5.39it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8321/8622 [15:04<00:55,  5.44it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8329/8622 [15:06<00:54,  5.37it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8337/8622 [15:07<00:51,  5.49it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8345/8622 [15:09<00:50,  5.54it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8353/8622 [15:10<00:51,  5.25it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8361/8622 [15:12<00:45,  5.75it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8369/8622 [15:12<00:35,  7.05it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8377/8622 [15:13<00:29,  8.43it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8385/8622 [15:14<00:28,  8.37it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8393/8622 [15:15<00:28,  8.05it/s]\u001B[A\n",
      "Evaluating:  97%|█████████▋| 8401/8622 [15:16<00:28,  7.85it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8409/8622 [15:16<00:25,  8.48it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8417/8622 [15:17<00:23,  8.54it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8425/8622 [15:18<00:21,  8.98it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8433/8622 [15:19<00:21,  8.89it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8441/8622 [15:20<00:20,  8.78it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8449/8622 [15:21<00:19,  8.66it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8457/8622 [15:22<00:18,  8.94it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8465/8622 [15:23<00:18,  8.37it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8473/8622 [15:24<00:17,  8.60it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8481/8622 [15:25<00:16,  8.57it/s]\u001B[A\n",
      "Evaluating:  98%|█████████▊| 8489/8622 [15:26<00:15,  8.40it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▊| 8497/8622 [15:26<00:13,  9.19it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▊| 8505/8622 [15:27<00:11,  9.85it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▊| 8513/8622 [15:28<00:12,  8.55it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8521/8622 [15:30<00:14,  7.12it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8529/8622 [15:31<00:13,  7.07it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8537/8622 [15:32<00:12,  6.81it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8545/8622 [15:33<00:10,  7.00it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8553/8622 [15:35<00:10,  6.50it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8561/8622 [15:38<00:14,  4.21it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8569/8622 [15:42<00:16,  3.24it/s]\u001B[A\n",
      "Evaluating:  99%|█████████▉| 8577/8622 [15:45<00:14,  3.20it/s]\u001B[A\n",
      "Evaluating: 100%|█████████▉| 8585/8622 [15:46<00:10,  3.64it/s]\u001B[A\n",
      "Evaluating: 100%|█████████▉| 8593/8622 [15:47<00:06,  4.32it/s]\u001B[A\n",
      "Evaluating: 100%|█████████▉| 8601/8622 [15:49<00:04,  4.67it/s]\u001B[A\n",
      "Evaluating: 100%|█████████▉| 8609/8622 [15:51<00:03,  4.26it/s]\u001B[A\n",
      "Evaluating: 100%|█████████▉| 8617/8622 [15:53<00:01,  4.03it/s]\u001B[A"
     ]
    }
   ],
   "source": [
    "model.freeze()\n",
    "predictions = list()\n",
    "predictions_indices = list()\n",
    "\n",
    "if mode == \"WSDisambiguator\":\n",
    "    progress_bar = tqdm(total=len(wic_dataset), desc='Evaluating')\n",
    "    for batch in DataLoader(wic_dataset, batch_size=8, collate_fn=WSDDataset.collate_fn):\n",
    "        out = model(batch)\n",
    "        predicted_indices, predicted_ids = model.predict(batch, out)\n",
    "        predictions += predicted_ids\n",
    "        predictions_indices += predicted_indices\n",
    "        progress_bar.update(len(batch))\n",
    "else:\n",
    "    tokens = list()\n",
    "    probs = list()\n",
    "    sense_indices = list()\n",
    "\n",
    "    progress_bar = tqdm(total=len(wic_dataset), desc='Evaluating')\n",
    "\n",
    "    dataloader = DataLoader(wic_dataset, batch_size=8, collate_fn=GlossBERTDataset.collate_fn)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        out = model(batch)\n",
    "        for i in range(len(batch[\"tokens\"])):\n",
    "            token = batch[\"tokens\"][i]\n",
    "            prob = out[\"probabilities\"][i]\n",
    "            sense_index = batch[\"sense_indices\"][i]\n",
    "            if len(tokens) > 0 and token == tokens[-1][-1]:\n",
    "                tokens[-1].append(token)\n",
    "                probs[-1] = torch.cat((probs[-1], torch.tensor([prob])))\n",
    "                sense_indices[-1] = torch.cat((sense_indices[-1], torch.tensor([sense_index])))\n",
    "            else:\n",
    "                tokens.append([token])\n",
    "                probs.append(torch.tensor([prob]))\n",
    "                sense_indices.append(torch.tensor([sense_index]))\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    for i in range(len(probs)):\n",
    "        sample_prediction_index = torch.argmax(probs[i])\n",
    "        predictions_indices.append(sense_indices[i][sample_prediction_index])\n",
    "        predictions.append(vocab_itos[predictions_indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "['superior%1:18:02::',\n 'superior%1:18:02::',\n 'superior%1:18:02::',\n 'superior%1:18:01::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:18:00::',\n 'baggage%1:06:00::',\n 'baggage%1:06:00::']"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_indices = torch.stack(predictions_indices)\n",
    "predictions[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['superior%1:18:01::',\n 'superior%1:18:02::',\n 'superior%1:18:01::',\n 'superior%1:18:01::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:18:00::',\n 'acquaintance%1:09:00::',\n 'acquaintance%1:18:00::',\n 'baggage%1:06:00::',\n 'baggage%1:06:00::']"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if mode == \"WSDisambiguator\":\n",
    "    gold = [vocab_itos[x[\"sense_index\"]] for x in wic_dataset]\n",
    "    gold_indices = torch.tensor([x[\"sense_index\"] for x in wic_dataset])\n",
    "else:\n",
    "    # wsd_gold_keys_path = constants.VALID_SET_PATH.replace(\"../..\", \"..\") + constants.TXT_GOLD_KEYS_SUFFIX\n",
    "    wsd_gold_keys_path = constants.WIC_TEST_SET_WSD_KEYS_PATH.replace(\"../..\", \"..\")\n",
    "    gold = list(read_wsd_gold_keys(wsd_gold_keys_path).values())\n",
    "    gold_indices = torch.tensor([vocab[x] for x in gold])\n",
    "gold[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'0.6010'"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsd_acc = accuracy_score(predictions, gold)\n",
    "f\"{wsd_acc:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wic_samples = read_wic_corpus(constants.WIC_TEST_SET_PATH.replace('../..', '..'), constants.WIC_TEST_SET_WSD_KEYS_PATH.replace('../..', '..'))\n",
    "gold_wic = [x.sense1.sense_id == x.sense2.sense_id for x in wic_samples]\n",
    "\n",
    "predictions_wic = [predictions[i] == predictions[i+1] for i in range(0, len(predictions), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "'0.6804'"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wic_acc = accuracy_score(predictions_wic, gold_wic)\n",
    "f\"{wic_acc:.4f}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _ = utils.plot_confusion_matrix(gold_wic, predictions_wic, normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "241c968aa5b2df334ebb21c90bd3ce900ec0c34e5bfa8ee6721f4d3c6b7b8eed"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('nlp2021-hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}